{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a8419c-ae2d-4665-9723-e111a8199862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802811ec-d1fb-404f-b92c-c99dcbf42d91",
   "metadata": {},
   "source": [
    "# Notes on Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8040584c-3432-4a3d-ae41-3bde89a88491",
   "metadata": {},
   "source": [
    "Although we expect random forest to perform better on our datasets as stated by the paper, when the dataset is low-dimensional and has less features, logisitc regression appears to perform slightly better. This is most likely due to the fact that in our Sepsis Dataset, there are only 3 features to go off of, meaking the data low-dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd2ee6d-b0cd-44ca-a940-f0ebe80a8a7b",
   "metadata": {},
   "source": [
    "# Occupation Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d845ae-02ac-4a40-97dc-87bf908d6a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-04 17:51:00</td>\n",
       "      <td>23.18</td>\n",
       "      <td>27.272</td>\n",
       "      <td>426</td>\n",
       "      <td>721.25</td>\n",
       "      <td>0.00479298817650529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-04 17:51:59</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2675</td>\n",
       "      <td>429.5</td>\n",
       "      <td>714</td>\n",
       "      <td>0.00478344094931065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-04 17:53:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.245</td>\n",
       "      <td>426</td>\n",
       "      <td>713.5</td>\n",
       "      <td>0.00477946352442199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-04 17:54:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2</td>\n",
       "      <td>426</td>\n",
       "      <td>708.25</td>\n",
       "      <td>0.00477150882608175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-04 17:55:00</td>\n",
       "      <td>23.1</td>\n",
       "      <td>27.2</td>\n",
       "      <td>426</td>\n",
       "      <td>704.5</td>\n",
       "      <td>0.00475699293331518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20557</th>\n",
       "      <td>2015-02-18 09:15:00</td>\n",
       "      <td>20.815</td>\n",
       "      <td>27.7175</td>\n",
       "      <td>429.75</td>\n",
       "      <td>1505.25</td>\n",
       "      <td>0.00421296819328694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20558</th>\n",
       "      <td>2015-02-18 09:16:00</td>\n",
       "      <td>20.865</td>\n",
       "      <td>27.745</td>\n",
       "      <td>423.5</td>\n",
       "      <td>1514.5</td>\n",
       "      <td>0.00423026193160229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20559</th>\n",
       "      <td>2015-02-18 09:16:59</td>\n",
       "      <td>20.89</td>\n",
       "      <td>27.745</td>\n",
       "      <td>423.5</td>\n",
       "      <td>1521.5</td>\n",
       "      <td>0.00423681810140671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20560</th>\n",
       "      <td>2015-02-18 09:17:59</td>\n",
       "      <td>20.89</td>\n",
       "      <td>28.0225</td>\n",
       "      <td>418.75</td>\n",
       "      <td>1632</td>\n",
       "      <td>0.0042794854718673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20561</th>\n",
       "      <td>2015-02-18 09:19:00</td>\n",
       "      <td>21</td>\n",
       "      <td>28.1</td>\n",
       "      <td>409</td>\n",
       "      <td>1864</td>\n",
       "      <td>0.00432073200293677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20562 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date Temperature Humidity   Light      CO2  \\\n",
       "0      2015-02-04 17:51:00       23.18   27.272     426   721.25   \n",
       "1      2015-02-04 17:51:59       23.15  27.2675   429.5      714   \n",
       "2      2015-02-04 17:53:00       23.15   27.245     426    713.5   \n",
       "3      2015-02-04 17:54:00       23.15     27.2     426   708.25   \n",
       "4      2015-02-04 17:55:00        23.1     27.2     426    704.5   \n",
       "...                    ...         ...      ...     ...      ...   \n",
       "20557  2015-02-18 09:15:00      20.815  27.7175  429.75  1505.25   \n",
       "20558  2015-02-18 09:16:00      20.865   27.745   423.5   1514.5   \n",
       "20559  2015-02-18 09:16:59       20.89   27.745   423.5   1521.5   \n",
       "20560  2015-02-18 09:17:59       20.89  28.0225  418.75     1632   \n",
       "20561  2015-02-18 09:19:00          21     28.1     409     1864   \n",
       "\n",
       "             HumidityRatio  \n",
       "0      0.00479298817650529  \n",
       "1      0.00478344094931065  \n",
       "2      0.00477946352442199  \n",
       "3      0.00477150882608175  \n",
       "4      0.00475699293331518  \n",
       "...                    ...  \n",
       "20557  0.00421296819328694  \n",
       "20558  0.00423026193160229  \n",
       "20559  0.00423681810140671  \n",
       "20560   0.0042794854718673  \n",
       "20561  0.00432073200293677  \n",
       "\n",
       "[20562 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20557</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20558</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20559</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20560</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20561</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20562 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Occupancy\n",
       "0            1.0\n",
       "1            1.0\n",
       "2            1.0\n",
       "3            1.0\n",
       "4            1.0\n",
       "...          ...\n",
       "20557        1.0\n",
       "20558        1.0\n",
       "20559        1.0\n",
       "20560        1.0\n",
       "20561        1.0\n",
       "\n",
       "[20562 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Occupancy\n",
    "occupancy_detection = fetch_ucirepo(id=357) \n",
    "X_occupancy = occupancy_detection.data.features \n",
    "y_occupancy = occupancy_detection.data.targets \n",
    "display(X_occupancy)\n",
    "display(y_occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d31fd6c7-c2ef-426d-a98c-ac96ab6e19e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-04 17:51:00</td>\n",
       "      <td>23.18</td>\n",
       "      <td>27.272</td>\n",
       "      <td>426</td>\n",
       "      <td>721.25</td>\n",
       "      <td>0.00479298817650529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-04 17:51:59</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2675</td>\n",
       "      <td>429.5</td>\n",
       "      <td>714</td>\n",
       "      <td>0.00478344094931065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-04 17:53:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.245</td>\n",
       "      <td>426</td>\n",
       "      <td>713.5</td>\n",
       "      <td>0.00477946352442199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-04 17:54:00</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2</td>\n",
       "      <td>426</td>\n",
       "      <td>708.25</td>\n",
       "      <td>0.00477150882608175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-04 17:55:00</td>\n",
       "      <td>23.1</td>\n",
       "      <td>27.2</td>\n",
       "      <td>426</td>\n",
       "      <td>704.5</td>\n",
       "      <td>0.00475699293331518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20557</th>\n",
       "      <td>2015-02-18 09:15:00</td>\n",
       "      <td>20.815</td>\n",
       "      <td>27.7175</td>\n",
       "      <td>429.75</td>\n",
       "      <td>1505.25</td>\n",
       "      <td>0.00421296819328694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20558</th>\n",
       "      <td>2015-02-18 09:16:00</td>\n",
       "      <td>20.865</td>\n",
       "      <td>27.745</td>\n",
       "      <td>423.5</td>\n",
       "      <td>1514.5</td>\n",
       "      <td>0.00423026193160229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20559</th>\n",
       "      <td>2015-02-18 09:16:59</td>\n",
       "      <td>20.89</td>\n",
       "      <td>27.745</td>\n",
       "      <td>423.5</td>\n",
       "      <td>1521.5</td>\n",
       "      <td>0.00423681810140671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20560</th>\n",
       "      <td>2015-02-18 09:17:59</td>\n",
       "      <td>20.89</td>\n",
       "      <td>28.0225</td>\n",
       "      <td>418.75</td>\n",
       "      <td>1632</td>\n",
       "      <td>0.0042794854718673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20561</th>\n",
       "      <td>2015-02-18 09:19:00</td>\n",
       "      <td>21</td>\n",
       "      <td>28.1</td>\n",
       "      <td>409</td>\n",
       "      <td>1864</td>\n",
       "      <td>0.00432073200293677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20560 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date Temperature Humidity   Light      CO2  \\\n",
       "0      2015-02-04 17:51:00       23.18   27.272     426   721.25   \n",
       "1      2015-02-04 17:51:59       23.15  27.2675   429.5      714   \n",
       "2      2015-02-04 17:53:00       23.15   27.245     426    713.5   \n",
       "3      2015-02-04 17:54:00       23.15     27.2     426   708.25   \n",
       "4      2015-02-04 17:55:00        23.1     27.2     426    704.5   \n",
       "...                    ...         ...      ...     ...      ...   \n",
       "20557  2015-02-18 09:15:00      20.815  27.7175  429.75  1505.25   \n",
       "20558  2015-02-18 09:16:00      20.865   27.745   423.5   1514.5   \n",
       "20559  2015-02-18 09:16:59       20.89   27.745   423.5   1521.5   \n",
       "20560  2015-02-18 09:17:59       20.89  28.0225  418.75     1632   \n",
       "20561  2015-02-18 09:19:00          21     28.1     409     1864   \n",
       "\n",
       "             HumidityRatio  \n",
       "0      0.00479298817650529  \n",
       "1      0.00478344094931065  \n",
       "2      0.00477946352442199  \n",
       "3      0.00477150882608175  \n",
       "4      0.00475699293331518  \n",
       "...                    ...  \n",
       "20557  0.00421296819328694  \n",
       "20558  0.00423026193160229  \n",
       "20559  0.00423681810140671  \n",
       "20560   0.0042794854718673  \n",
       "20561  0.00432073200293677  \n",
       "\n",
       "[20560 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20557</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20558</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20559</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20560</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20561</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20560 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Occupancy\n",
       "0            1.0\n",
       "1            1.0\n",
       "2            1.0\n",
       "3            1.0\n",
       "4            1.0\n",
       "...          ...\n",
       "20557        1.0\n",
       "20558        1.0\n",
       "20559        1.0\n",
       "20560        1.0\n",
       "20561        1.0\n",
       "\n",
       "[20560 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#mismatched data in certain rows where it says 'Temperature' instead of the date. Identify and drop the corresponding rows\n",
    "temperature_rows = X_occupancy[X_occupancy['date'].str.contains('Temperature', na=False)]\n",
    "\n",
    "X_occupancy = X_occupancy.drop(temperature_rows.index) #drops mismatched types\n",
    "y_occupancy = y_occupancy.drop(temperature_rows.index) #drops the corresponding mistmatched types in the gt\n",
    "display(X_occupancy)\n",
    "display(y_occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "118280eb-0766-4540-9b3e-de1423ec593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   date_only  time_only\n",
      "0     735633      64260\n",
      "1     735633      64319\n",
      "2     735633      64380\n",
      "3     735633      64440\n",
      "4     735633      64500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "      <th>date_only</th>\n",
       "      <th>time_only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.18</td>\n",
       "      <td>27.272</td>\n",
       "      <td>426</td>\n",
       "      <td>721.25</td>\n",
       "      <td>0.00479298817650529</td>\n",
       "      <td>735633</td>\n",
       "      <td>64260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2675</td>\n",
       "      <td>429.5</td>\n",
       "      <td>714</td>\n",
       "      <td>0.00478344094931065</td>\n",
       "      <td>735633</td>\n",
       "      <td>64319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.15</td>\n",
       "      <td>27.245</td>\n",
       "      <td>426</td>\n",
       "      <td>713.5</td>\n",
       "      <td>0.00477946352442199</td>\n",
       "      <td>735633</td>\n",
       "      <td>64380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2</td>\n",
       "      <td>426</td>\n",
       "      <td>708.25</td>\n",
       "      <td>0.00477150882608175</td>\n",
       "      <td>735633</td>\n",
       "      <td>64440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.1</td>\n",
       "      <td>27.2</td>\n",
       "      <td>426</td>\n",
       "      <td>704.5</td>\n",
       "      <td>0.00475699293331518</td>\n",
       "      <td>735633</td>\n",
       "      <td>64500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20557</th>\n",
       "      <td>20.815</td>\n",
       "      <td>27.7175</td>\n",
       "      <td>429.75</td>\n",
       "      <td>1505.25</td>\n",
       "      <td>0.00421296819328694</td>\n",
       "      <td>735647</td>\n",
       "      <td>33300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20558</th>\n",
       "      <td>20.865</td>\n",
       "      <td>27.745</td>\n",
       "      <td>423.5</td>\n",
       "      <td>1514.5</td>\n",
       "      <td>0.00423026193160229</td>\n",
       "      <td>735647</td>\n",
       "      <td>33360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20559</th>\n",
       "      <td>20.89</td>\n",
       "      <td>27.745</td>\n",
       "      <td>423.5</td>\n",
       "      <td>1521.5</td>\n",
       "      <td>0.00423681810140671</td>\n",
       "      <td>735647</td>\n",
       "      <td>33419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20560</th>\n",
       "      <td>20.89</td>\n",
       "      <td>28.0225</td>\n",
       "      <td>418.75</td>\n",
       "      <td>1632</td>\n",
       "      <td>0.0042794854718673</td>\n",
       "      <td>735647</td>\n",
       "      <td>33479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20561</th>\n",
       "      <td>21</td>\n",
       "      <td>28.1</td>\n",
       "      <td>409</td>\n",
       "      <td>1864</td>\n",
       "      <td>0.00432073200293677</td>\n",
       "      <td>735647</td>\n",
       "      <td>33540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20560 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Temperature Humidity   Light      CO2        HumidityRatio  date_only  \\\n",
       "0           23.18   27.272     426   721.25  0.00479298817650529     735633   \n",
       "1           23.15  27.2675   429.5      714  0.00478344094931065     735633   \n",
       "2           23.15   27.245     426    713.5  0.00477946352442199     735633   \n",
       "3           23.15     27.2     426   708.25  0.00477150882608175     735633   \n",
       "4            23.1     27.2     426    704.5  0.00475699293331518     735633   \n",
       "...           ...      ...     ...      ...                  ...        ...   \n",
       "20557      20.815  27.7175  429.75  1505.25  0.00421296819328694     735647   \n",
       "20558      20.865   27.745   423.5   1514.5  0.00423026193160229     735647   \n",
       "20559       20.89   27.745   423.5   1521.5  0.00423681810140671     735647   \n",
       "20560       20.89  28.0225  418.75     1632   0.0042794854718673     735647   \n",
       "20561          21     28.1     409     1864  0.00432073200293677     735647   \n",
       "\n",
       "       time_only  \n",
       "0          64260  \n",
       "1          64319  \n",
       "2          64380  \n",
       "3          64440  \n",
       "4          64500  \n",
       "...          ...  \n",
       "20557      33300  \n",
       "20558      33360  \n",
       "20559      33419  \n",
       "20560      33479  \n",
       "20561      33540  \n",
       "\n",
       "[20560 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20557</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20558</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20559</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20560</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20561</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20560 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Occupancy\n",
       "0            1.0\n",
       "1            1.0\n",
       "2            1.0\n",
       "3            1.0\n",
       "4            1.0\n",
       "...          ...\n",
       "20557        1.0\n",
       "20558        1.0\n",
       "20559        1.0\n",
       "20560        1.0\n",
       "20561        1.0\n",
       "\n",
       "[20560 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Ensure the 'date' column is in datetime format\n",
    "X_occupancy['date'] = pd.to_datetime(X_occupancy.loc[:,'date'], errors='coerce')\n",
    "\n",
    "# # Split the 'date' column into 'date' and 'time' columns\n",
    "X_occupancy['date_only'] = X_occupancy.loc[:,'date'].dt.date  # Extract just the date (YYYY-MM-DD)\n",
    "X_occupancy['time_only'] = X_occupancy.loc[:,'date'].dt.time  # Extract just the time (HH:MM:SS)\n",
    "\n",
    "# Convert date to ordinal\n",
    "X_occupancy['date_only'] = X_occupancy.loc[:,'date_only'].map(lambda x: x.toordinal())\n",
    "\n",
    "# Convert time to seconds since midnight\n",
    "X_occupancy['time_only'] = X_occupancy.loc[:,'time_only'].map(\n",
    "    lambda x: x.hour * 3600 + x.minute * 60 + x.second\n",
    ")\n",
    "\n",
    "X_occupancy = X_occupancy.drop('date', axis = 1)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(X_occupancy[['date_only', 'time_only']].head())\n",
    "display(X_occupancy)\n",
    "display(y_occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7079c3-ee04-46ee-b3e1-ebd98f468aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Temperature Humidity   Light      CO2        HumidityRatio  date_only  \\\n",
      "0           23.18   27.272     426   721.25  0.00479298817650529     735633   \n",
      "1           23.15  27.2675   429.5      714  0.00478344094931065     735633   \n",
      "2           23.15   27.245     426    713.5  0.00477946352442199     735633   \n",
      "3           23.15     27.2     426   708.25  0.00477150882608175     735633   \n",
      "4            23.1     27.2     426    704.5  0.00475699293331518     735633   \n",
      "...           ...      ...     ...      ...                  ...        ...   \n",
      "20557      20.815  27.7175  429.75  1505.25  0.00421296819328694     735647   \n",
      "20558      20.865   27.745   423.5   1514.5  0.00423026193160229     735647   \n",
      "20559       20.89   27.745   423.5   1521.5  0.00423681810140671     735647   \n",
      "20560       20.89  28.0225  418.75     1632   0.0042794854718673     735647   \n",
      "20561          21     28.1     409     1864  0.00432073200293677     735647   \n",
      "\n",
      "       time_only  \n",
      "0          64260  \n",
      "1          64319  \n",
      "2          64380  \n",
      "3          64440  \n",
      "4          64500  \n",
      "...          ...  \n",
      "20557      33300  \n",
      "20558      33360  \n",
      "20559      33419  \n",
      "20560      33479  \n",
      "20561      33540  \n",
      "\n",
      "[20560 rows x 7 columns]\n",
      "Temperature      float64\n",
      "Humidity         float64\n",
      "Light            float64\n",
      "CO2              float64\n",
      "HumidityRatio    float64\n",
      "date_only        float64\n",
      "time_only        float64\n",
      "dtype: object\n",
      "       Temperature  Humidity   Light      CO2  HumidityRatio  date_only  \\\n",
      "0           23.180   27.2720  426.00   721.25       0.004793   735633.0   \n",
      "1           23.150   27.2675  429.50   714.00       0.004783   735633.0   \n",
      "2           23.150   27.2450  426.00   713.50       0.004779   735633.0   \n",
      "3           23.150   27.2000  426.00   708.25       0.004772   735633.0   \n",
      "4           23.100   27.2000  426.00   704.50       0.004757   735633.0   \n",
      "...            ...       ...     ...      ...            ...        ...   \n",
      "20557       20.815   27.7175  429.75  1505.25       0.004213   735647.0   \n",
      "20558       20.865   27.7450  423.50  1514.50       0.004230   735647.0   \n",
      "20559       20.890   27.7450  423.50  1521.50       0.004237   735647.0   \n",
      "20560       20.890   28.0225  418.75  1632.00       0.004279   735647.0   \n",
      "20561       21.000   28.1000  409.00  1864.00       0.004321   735647.0   \n",
      "\n",
      "       time_only  \n",
      "0        64260.0  \n",
      "1        64319.0  \n",
      "2        64380.0  \n",
      "3        64440.0  \n",
      "4        64500.0  \n",
      "...          ...  \n",
      "20557    33300.0  \n",
      "20558    33360.0  \n",
      "20559    33419.0  \n",
      "20560    33479.0  \n",
      "20561    33540.0  \n",
      "\n",
      "[20560 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "non_numeric_rows = X_occupancy[X_occupancy.map(lambda x: not isinstance(x, (int, float))).any(axis=1)]\n",
    "print(non_numeric_rows)\n",
    "X_occupancy = X_occupancy.astype(float)\n",
    "y_occupancy = y_occupancy.astype(int)\n",
    "\n",
    "print(X_occupancy.dtypes)\n",
    "print(X_occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4969a6e7-e9d2-4c87-9dee-26d966b81177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature preview (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "      <th>date_only</th>\n",
       "      <th>time_only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.154659</td>\n",
       "      <td>-0.077062</td>\n",
       "      <td>1.403076</td>\n",
       "      <td>0.098642</td>\n",
       "      <td>0.735396</td>\n",
       "      <td>-1.284171</td>\n",
       "      <td>0.833914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.126231</td>\n",
       "      <td>-0.077965</td>\n",
       "      <td>1.419709</td>\n",
       "      <td>0.075344</td>\n",
       "      <td>0.722962</td>\n",
       "      <td>-1.284171</td>\n",
       "      <td>0.836228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.126231</td>\n",
       "      <td>-0.082481</td>\n",
       "      <td>1.403076</td>\n",
       "      <td>0.073738</td>\n",
       "      <td>0.717782</td>\n",
       "      <td>-1.284171</td>\n",
       "      <td>0.838620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.126231</td>\n",
       "      <td>-0.091514</td>\n",
       "      <td>1.403076</td>\n",
       "      <td>0.056867</td>\n",
       "      <td>0.707423</td>\n",
       "      <td>-1.284171</td>\n",
       "      <td>0.840974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.078850</td>\n",
       "      <td>-0.091514</td>\n",
       "      <td>1.403076</td>\n",
       "      <td>0.044817</td>\n",
       "      <td>0.688518</td>\n",
       "      <td>-1.284171</td>\n",
       "      <td>0.843327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature  Humidity     Light       CO2  HumidityRatio  date_only  \\\n",
       "0     2.154659 -0.077062  1.403076  0.098642       0.735396  -1.284171   \n",
       "1     2.126231 -0.077965  1.419709  0.075344       0.722962  -1.284171   \n",
       "2     2.126231 -0.082481  1.403076  0.073738       0.717782  -1.284171   \n",
       "3     2.126231 -0.091514  1.403076  0.056867       0.707423  -1.284171   \n",
       "4     2.078850 -0.091514  1.403076  0.044817       0.688518  -1.284171   \n",
       "\n",
       "   time_only  \n",
       "0   0.833914  \n",
       "1   0.836228  \n",
       "2   0.838620  \n",
       "3   0.840974  \n",
       "4   0.843327  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling\n",
    "X_occupancy = pd.DataFrame(scaler.fit_transform(X_occupancy), columns=X_occupancy.columns)\n",
    "\n",
    "# Verify the scaling\n",
    "print(\"Scaled feature preview (first 5 rows):\")\n",
    "display(X_occupancy.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b0581c3-5fa5-4758-839b-d06c652915ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20557</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20558</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20559</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20560</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20561</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20560 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Occupancy\n",
       "0              1\n",
       "1              1\n",
       "2              1\n",
       "3              1\n",
       "4              1\n",
       "...          ...\n",
       "20557          1\n",
       "20558          1\n",
       "20559          1\n",
       "20560          1\n",
       "20561          1\n",
       "\n",
       "[20560 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_occupancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c387a964-ed04-482c-8f96-4f9cbc70bcc2",
   "metadata": {},
   "source": [
    "# Sepsis Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dfc70c3-22cc-4e2e-a776-c44cec453ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_years</th>\n",
       "      <th>sex_0male_1female</th>\n",
       "      <th>episode_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110336</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110337</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110338</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110339</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110340</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110341 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age_years  sex_0male_1female  episode_number\n",
       "0              21                  1               1\n",
       "1              20                  1               1\n",
       "2              21                  1               1\n",
       "3              77                  0               1\n",
       "4              72                  0               1\n",
       "...           ...                ...             ...\n",
       "110336         47                  0               1\n",
       "110337         50                  0               1\n",
       "110338         62                  0               1\n",
       "110339         58                  0               1\n",
       "110340         55                  1               1\n",
       "\n",
       "[110341 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospital_outcome_1alive_0dead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110336</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110337</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110338</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110339</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110340</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110341 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hospital_outcome_1alive_0dead\n",
       "0                                   1\n",
       "1                                   1\n",
       "2                                   1\n",
       "3                                   1\n",
       "4                                   1\n",
       "...                               ...\n",
       "110336                              1\n",
       "110337                              0\n",
       "110338                              1\n",
       "110339                              0\n",
       "110340                              1\n",
       "\n",
       "[110341 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fetch dataset \n",
    "sepsis_survival_minimal_clinical_records = fetch_ucirepo(id=827) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_sepsis = sepsis_survival_minimal_clinical_records.data.features \n",
    "y_sepsis = sepsis_survival_minimal_clinical_records.data.targets \n",
    "  \n",
    "# metadata \n",
    "# print(sepsis_survival_minimal_clinical_records.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(sepsis_survival_minimal_clinical_records.variables) \n",
    "# y_sepsis.loc[:, \"hospital_outcome_1alive_0dead\"] = np.where(y_sepsis[\"hospital_outcome_1alive_0dead\"] == 0, -1, 1) #-1 represents empty and 1 represents occupied\n",
    "display(X_sepsis)\n",
    "display(y_sepsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe39d64-8cc4-4b53-bcf0-7bb61b1f2812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age_years  sex_0male_1female  episode_number\n",
      "0       -1.730274           1.053878       -0.464617\n",
      "1       -1.771736           1.053878       -0.464617\n",
      "2       -1.730274           1.053878       -0.464617\n",
      "3        0.591613          -0.948877       -0.464617\n",
      "4        0.384302          -0.948877       -0.464617\n",
      "...           ...                ...             ...\n",
      "110336  -0.652255          -0.948877       -0.464617\n",
      "110337  -0.527868          -0.948877       -0.464617\n",
      "110338  -0.030321          -0.948877       -0.464617\n",
      "110339  -0.196170          -0.948877       -0.464617\n",
      "110340  -0.320557           1.053878       -0.464617\n",
      "\n",
      "[110341 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Scale the data\n",
    "X_sepsis = pd.DataFrame(scaler.fit_transform(X_sepsis), columns=X_sepsis.columns)\n",
    "print(X_sepsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9f275-2550-4cd7-97fa-a882780f153d",
   "metadata": {},
   "source": [
    "# Coupon Acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "157da73e-73a5-452e-8385-5b59e4b635c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination</th>\n",
       "      <th>passenger</th>\n",
       "      <th>weather</th>\n",
       "      <th>temperature</th>\n",
       "      <th>time</th>\n",
       "      <th>coupon</th>\n",
       "      <th>expiration</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>maritalStatus</th>\n",
       "      <th>...</th>\n",
       "      <th>Bar</th>\n",
       "      <th>CoffeeHouse</th>\n",
       "      <th>CarryAway</th>\n",
       "      <th>RestaurantLessThan20</th>\n",
       "      <th>Restaurant20To50</th>\n",
       "      <th>toCoupon_GEQ5min</th>\n",
       "      <th>toCoupon_GEQ15min</th>\n",
       "      <th>toCoupon_GEQ25min</th>\n",
       "      <th>direction_same</th>\n",
       "      <th>direction_opp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Alone</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>55</td>\n",
       "      <td>2PM</td>\n",
       "      <td>Restaurant(&lt;20)</td>\n",
       "      <td>1d</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>...</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Friend(s)</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>10AM</td>\n",
       "      <td>Coffee House</td>\n",
       "      <td>2h</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>...</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Friend(s)</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>10AM</td>\n",
       "      <td>Carry out &amp; Take away</td>\n",
       "      <td>2h</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>...</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Friend(s)</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>2PM</td>\n",
       "      <td>Coffee House</td>\n",
       "      <td>2h</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>...</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Urgent Place</td>\n",
       "      <td>Friend(s)</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>2PM</td>\n",
       "      <td>Coffee House</td>\n",
       "      <td>1d</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>Unmarried partner</td>\n",
       "      <td>...</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12679</th>\n",
       "      <td>Home</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Rainy</td>\n",
       "      <td>55</td>\n",
       "      <td>6PM</td>\n",
       "      <td>Carry out &amp; Take away</td>\n",
       "      <td>1d</td>\n",
       "      <td>Male</td>\n",
       "      <td>26</td>\n",
       "      <td>Single</td>\n",
       "      <td>...</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>1~3</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>Work</td>\n",
       "      <td>Alone</td>\n",
       "      <td>Rainy</td>\n",
       "      <td>55</td>\n",
       "      <td>7AM</td>\n",
       "      <td>Carry out &amp; Take away</td>\n",
       "      <td>1d</td>\n",
       "      <td>Male</td>\n",
       "      <td>26</td>\n",
       "      <td>Single</td>\n",
       "      <td>...</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>1~3</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12681</th>\n",
       "      <td>Work</td>\n",
       "      <td>Alone</td>\n",
       "      <td>Snowy</td>\n",
       "      <td>30</td>\n",
       "      <td>7AM</td>\n",
       "      <td>Coffee House</td>\n",
       "      <td>1d</td>\n",
       "      <td>Male</td>\n",
       "      <td>26</td>\n",
       "      <td>Single</td>\n",
       "      <td>...</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>1~3</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12682</th>\n",
       "      <td>Work</td>\n",
       "      <td>Alone</td>\n",
       "      <td>Snowy</td>\n",
       "      <td>30</td>\n",
       "      <td>7AM</td>\n",
       "      <td>Bar</td>\n",
       "      <td>1d</td>\n",
       "      <td>Male</td>\n",
       "      <td>26</td>\n",
       "      <td>Single</td>\n",
       "      <td>...</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>1~3</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12683</th>\n",
       "      <td>Work</td>\n",
       "      <td>Alone</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>7AM</td>\n",
       "      <td>Restaurant(20-50)</td>\n",
       "      <td>2h</td>\n",
       "      <td>Male</td>\n",
       "      <td>26</td>\n",
       "      <td>Single</td>\n",
       "      <td>...</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "      <td>1~3</td>\n",
       "      <td>4~8</td>\n",
       "      <td>1~3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12684 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           destination  passenger weather  temperature  time  \\\n",
       "0      No Urgent Place      Alone   Sunny           55   2PM   \n",
       "1      No Urgent Place  Friend(s)   Sunny           80  10AM   \n",
       "2      No Urgent Place  Friend(s)   Sunny           80  10AM   \n",
       "3      No Urgent Place  Friend(s)   Sunny           80   2PM   \n",
       "4      No Urgent Place  Friend(s)   Sunny           80   2PM   \n",
       "...                ...        ...     ...          ...   ...   \n",
       "12679             Home    Partner   Rainy           55   6PM   \n",
       "12680             Work      Alone   Rainy           55   7AM   \n",
       "12681             Work      Alone   Snowy           30   7AM   \n",
       "12682             Work      Alone   Snowy           30   7AM   \n",
       "12683             Work      Alone   Sunny           80   7AM   \n",
       "\n",
       "                      coupon expiration  gender age      maritalStatus  ...  \\\n",
       "0            Restaurant(<20)         1d  Female  21  Unmarried partner  ...   \n",
       "1               Coffee House         2h  Female  21  Unmarried partner  ...   \n",
       "2      Carry out & Take away         2h  Female  21  Unmarried partner  ...   \n",
       "3               Coffee House         2h  Female  21  Unmarried partner  ...   \n",
       "4               Coffee House         1d  Female  21  Unmarried partner  ...   \n",
       "...                      ...        ...     ...  ..                ...  ...   \n",
       "12679  Carry out & Take away         1d    Male  26             Single  ...   \n",
       "12680  Carry out & Take away         1d    Male  26             Single  ...   \n",
       "12681           Coffee House         1d    Male  26             Single  ...   \n",
       "12682                    Bar         1d    Male  26             Single  ...   \n",
       "12683      Restaurant(20-50)         2h    Male  26             Single  ...   \n",
       "\n",
       "         Bar CoffeeHouse CarryAway RestaurantLessThan20 Restaurant20To50  \\\n",
       "0      never       never       NaN                  4~8              1~3   \n",
       "1      never       never       NaN                  4~8              1~3   \n",
       "2      never       never       NaN                  4~8              1~3   \n",
       "3      never       never       NaN                  4~8              1~3   \n",
       "4      never       never       NaN                  4~8              1~3   \n",
       "...      ...         ...       ...                  ...              ...   \n",
       "12679  never       never       1~3                  4~8              1~3   \n",
       "12680  never       never       1~3                  4~8              1~3   \n",
       "12681  never       never       1~3                  4~8              1~3   \n",
       "12682  never       never       1~3                  4~8              1~3   \n",
       "12683  never       never       1~3                  4~8              1~3   \n",
       "\n",
       "      toCoupon_GEQ5min toCoupon_GEQ15min toCoupon_GEQ25min direction_same  \\\n",
       "0                    1                 0                 0              0   \n",
       "1                    1                 0                 0              0   \n",
       "2                    1                 1                 0              0   \n",
       "3                    1                 1                 0              0   \n",
       "4                    1                 1                 0              0   \n",
       "...                ...               ...               ...            ...   \n",
       "12679                1                 0                 0              1   \n",
       "12680                1                 0                 0              0   \n",
       "12681                1                 0                 0              1   \n",
       "12682                1                 1                 1              0   \n",
       "12683                1                 0                 0              1   \n",
       "\n",
       "      direction_opp  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  \n",
       "...             ...  \n",
       "12679             0  \n",
       "12680             1  \n",
       "12681             0  \n",
       "12682             1  \n",
       "12683             0  \n",
       "\n",
       "[12684 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12679</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12681</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12682</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12683</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12684 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Y\n",
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      0\n",
       "...   ..\n",
       "12679  1\n",
       "12680  1\n",
       "12681  0\n",
       "12682  0\n",
       "12683  0\n",
       "\n",
       "[12684 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Used to handle the non numerical age values\n",
    "def transform_age(age):\n",
    "    if age == '50plus':\n",
    "        return 50\n",
    "    elif age == 'below21':\n",
    "        return 20\n",
    "    else:\n",
    "        return int(age) \n",
    "        \n",
    "#Coupon dataset\n",
    "in_vehicle_coupon_recommendation = fetch_ucirepo(id=603) \n",
    "X_coupons = in_vehicle_coupon_recommendation.data.features \n",
    "y_coupons = in_vehicle_coupon_recommendation.data.targets \n",
    "display(X_coupons)\n",
    "display(y_coupons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a663868c-39fa-41e3-a010-c34447937dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>age</th>\n",
       "      <th>has_children</th>\n",
       "      <th>toCoupon_GEQ5min</th>\n",
       "      <th>toCoupon_GEQ15min</th>\n",
       "      <th>toCoupon_GEQ25min</th>\n",
       "      <th>direction_same</th>\n",
       "      <th>direction_opp</th>\n",
       "      <th>destination_Home</th>\n",
       "      <th>destination_No Urgent Place</th>\n",
       "      <th>...</th>\n",
       "      <th>RestaurantLessThan20_1~3</th>\n",
       "      <th>RestaurantLessThan20_4~8</th>\n",
       "      <th>RestaurantLessThan20_gt8</th>\n",
       "      <th>RestaurantLessThan20_less1</th>\n",
       "      <th>RestaurantLessThan20_never</th>\n",
       "      <th>Restaurant20To50_1~3</th>\n",
       "      <th>Restaurant20To50_4~8</th>\n",
       "      <th>Restaurant20To50_gt8</th>\n",
       "      <th>Restaurant20To50_less1</th>\n",
       "      <th>Restaurant20To50_never</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12679</th>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12680</th>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12681</th>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12682</th>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12683</th>\n",
       "      <td>80</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12684 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       temperature age  has_children  toCoupon_GEQ5min  toCoupon_GEQ15min  \\\n",
       "0               55  21             1                 1                  0   \n",
       "1               80  21             1                 1                  0   \n",
       "2               80  21             1                 1                  1   \n",
       "3               80  21             1                 1                  1   \n",
       "4               80  21             1                 1                  1   \n",
       "...            ...  ..           ...               ...                ...   \n",
       "12679           55  26             0                 1                  0   \n",
       "12680           55  26             0                 1                  0   \n",
       "12681           30  26             0                 1                  0   \n",
       "12682           30  26             0                 1                  1   \n",
       "12683           80  26             0                 1                  0   \n",
       "\n",
       "       toCoupon_GEQ25min  direction_same  direction_opp  destination_Home  \\\n",
       "0                      0               0              1             False   \n",
       "1                      0               0              1             False   \n",
       "2                      0               0              1             False   \n",
       "3                      0               0              1             False   \n",
       "4                      0               0              1             False   \n",
       "...                  ...             ...            ...               ...   \n",
       "12679                  0               1              0              True   \n",
       "12680                  0               0              1             False   \n",
       "12681                  0               1              0             False   \n",
       "12682                  1               0              1             False   \n",
       "12683                  0               1              0             False   \n",
       "\n",
       "       destination_No Urgent Place  ...  RestaurantLessThan20_1~3  \\\n",
       "0                             True  ...                     False   \n",
       "1                             True  ...                     False   \n",
       "2                             True  ...                     False   \n",
       "3                             True  ...                     False   \n",
       "4                             True  ...                     False   \n",
       "...                            ...  ...                       ...   \n",
       "12679                        False  ...                     False   \n",
       "12680                        False  ...                     False   \n",
       "12681                        False  ...                     False   \n",
       "12682                        False  ...                     False   \n",
       "12683                        False  ...                     False   \n",
       "\n",
       "       RestaurantLessThan20_4~8  RestaurantLessThan20_gt8  \\\n",
       "0                          True                     False   \n",
       "1                          True                     False   \n",
       "2                          True                     False   \n",
       "3                          True                     False   \n",
       "4                          True                     False   \n",
       "...                         ...                       ...   \n",
       "12679                      True                     False   \n",
       "12680                      True                     False   \n",
       "12681                      True                     False   \n",
       "12682                      True                     False   \n",
       "12683                      True                     False   \n",
       "\n",
       "       RestaurantLessThan20_less1  RestaurantLessThan20_never  \\\n",
       "0                           False                       False   \n",
       "1                           False                       False   \n",
       "2                           False                       False   \n",
       "3                           False                       False   \n",
       "4                           False                       False   \n",
       "...                           ...                         ...   \n",
       "12679                       False                       False   \n",
       "12680                       False                       False   \n",
       "12681                       False                       False   \n",
       "12682                       False                       False   \n",
       "12683                       False                       False   \n",
       "\n",
       "       Restaurant20To50_1~3  Restaurant20To50_4~8  Restaurant20To50_gt8  \\\n",
       "0                      True                 False                 False   \n",
       "1                      True                 False                 False   \n",
       "2                      True                 False                 False   \n",
       "3                      True                 False                 False   \n",
       "4                      True                 False                 False   \n",
       "...                     ...                   ...                   ...   \n",
       "12679                  True                 False                 False   \n",
       "12680                  True                 False                 False   \n",
       "12681                  True                 False                 False   \n",
       "12682                  True                 False                 False   \n",
       "12683                  True                 False                 False   \n",
       "\n",
       "       Restaurant20To50_less1  Restaurant20To50_never  \n",
       "0                       False                   False  \n",
       "1                       False                   False  \n",
       "2                       False                   False  \n",
       "3                       False                   False  \n",
       "4                       False                   False  \n",
       "...                       ...                     ...  \n",
       "12679                   False                   False  \n",
       "12680                   False                   False  \n",
       "12681                   False                   False  \n",
       "12682                   False                   False  \n",
       "12683                   False                   False  \n",
       "\n",
       "[12684 rows x 107 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# need to apply one-hot encoding to categorical data\n",
    "categorical_columns = [\n",
    "    'destination', 'passenger', 'weather', 'time', 'coupon', 'expiration', \n",
    "    'gender', 'maritalStatus', 'education', 'occupation', 'income', 'car', \n",
    "    'Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50'\n",
    "]\n",
    "X_coupons = pd.get_dummies(X_coupons, columns=categorical_columns)\n",
    "display(X_coupons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40cd9cf6-6a1d-4b86-bee7-d0d2d978e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to properly encode the age as integers\n",
    "X_coupons.loc[:,'age'] = X_coupons.loc[:,'age'].apply(transform_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03d5bfa8-d3fe-44b4-93ad-1f896e396b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled feature preview (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>age</th>\n",
       "      <th>has_children</th>\n",
       "      <th>toCoupon_GEQ5min</th>\n",
       "      <th>toCoupon_GEQ15min</th>\n",
       "      <th>toCoupon_GEQ25min</th>\n",
       "      <th>direction_same</th>\n",
       "      <th>direction_opp</th>\n",
       "      <th>destination_Home</th>\n",
       "      <th>destination_No Urgent Place</th>\n",
       "      <th>...</th>\n",
       "      <th>RestaurantLessThan20_1~3</th>\n",
       "      <th>RestaurantLessThan20_4~8</th>\n",
       "      <th>RestaurantLessThan20_gt8</th>\n",
       "      <th>RestaurantLessThan20_less1</th>\n",
       "      <th>RestaurantLessThan20_never</th>\n",
       "      <th>Restaurant20To50_1~3</th>\n",
       "      <th>Restaurant20To50_4~8</th>\n",
       "      <th>Restaurant20To50_gt8</th>\n",
       "      <th>Restaurant20To50_less1</th>\n",
       "      <th>Restaurant20To50_never</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.433430</td>\n",
       "      <td>-1.108935</td>\n",
       "      <td>1.189378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.131581</td>\n",
       "      <td>-0.367745</td>\n",
       "      <td>-0.522967</td>\n",
       "      <td>0.522967</td>\n",
       "      <td>-0.585362</td>\n",
       "      <td>1.009347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.85769</td>\n",
       "      <td>1.594684</td>\n",
       "      <td>-0.335752</td>\n",
       "      <td>-0.444545</td>\n",
       "      <td>-0.132856</td>\n",
       "      <td>1.689769</td>\n",
       "      <td>-0.246759</td>\n",
       "      <td>-0.145795</td>\n",
       "      <td>-0.959053</td>\n",
       "      <td>-0.450003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.871799</td>\n",
       "      <td>-1.108935</td>\n",
       "      <td>1.189378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.131581</td>\n",
       "      <td>-0.367745</td>\n",
       "      <td>-0.522967</td>\n",
       "      <td>0.522967</td>\n",
       "      <td>-0.585362</td>\n",
       "      <td>1.009347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.85769</td>\n",
       "      <td>1.594684</td>\n",
       "      <td>-0.335752</td>\n",
       "      <td>-0.444545</td>\n",
       "      <td>-0.132856</td>\n",
       "      <td>1.689769</td>\n",
       "      <td>-0.246759</td>\n",
       "      <td>-0.145795</td>\n",
       "      <td>-0.959053</td>\n",
       "      <td>-0.450003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.871799</td>\n",
       "      <td>-1.108935</td>\n",
       "      <td>1.189378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.883720</td>\n",
       "      <td>-0.367745</td>\n",
       "      <td>-0.522967</td>\n",
       "      <td>0.522967</td>\n",
       "      <td>-0.585362</td>\n",
       "      <td>1.009347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.85769</td>\n",
       "      <td>1.594684</td>\n",
       "      <td>-0.335752</td>\n",
       "      <td>-0.444545</td>\n",
       "      <td>-0.132856</td>\n",
       "      <td>1.689769</td>\n",
       "      <td>-0.246759</td>\n",
       "      <td>-0.145795</td>\n",
       "      <td>-0.959053</td>\n",
       "      <td>-0.450003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.871799</td>\n",
       "      <td>-1.108935</td>\n",
       "      <td>1.189378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.883720</td>\n",
       "      <td>-0.367745</td>\n",
       "      <td>-0.522967</td>\n",
       "      <td>0.522967</td>\n",
       "      <td>-0.585362</td>\n",
       "      <td>1.009347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.85769</td>\n",
       "      <td>1.594684</td>\n",
       "      <td>-0.335752</td>\n",
       "      <td>-0.444545</td>\n",
       "      <td>-0.132856</td>\n",
       "      <td>1.689769</td>\n",
       "      <td>-0.246759</td>\n",
       "      <td>-0.145795</td>\n",
       "      <td>-0.959053</td>\n",
       "      <td>-0.450003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.871799</td>\n",
       "      <td>-1.108935</td>\n",
       "      <td>1.189378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.883720</td>\n",
       "      <td>-0.367745</td>\n",
       "      <td>-0.522967</td>\n",
       "      <td>0.522967</td>\n",
       "      <td>-0.585362</td>\n",
       "      <td>1.009347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.85769</td>\n",
       "      <td>1.594684</td>\n",
       "      <td>-0.335752</td>\n",
       "      <td>-0.444545</td>\n",
       "      <td>-0.132856</td>\n",
       "      <td>1.689769</td>\n",
       "      <td>-0.246759</td>\n",
       "      <td>-0.145795</td>\n",
       "      <td>-0.959053</td>\n",
       "      <td>-0.450003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   temperature       age  has_children  toCoupon_GEQ5min  toCoupon_GEQ15min  \\\n",
       "0    -0.433430 -1.108935      1.189378               0.0          -1.131581   \n",
       "1     0.871799 -1.108935      1.189378               0.0          -1.131581   \n",
       "2     0.871799 -1.108935      1.189378               0.0           0.883720   \n",
       "3     0.871799 -1.108935      1.189378               0.0           0.883720   \n",
       "4     0.871799 -1.108935      1.189378               0.0           0.883720   \n",
       "\n",
       "   toCoupon_GEQ25min  direction_same  direction_opp  destination_Home  \\\n",
       "0          -0.367745       -0.522967       0.522967         -0.585362   \n",
       "1          -0.367745       -0.522967       0.522967         -0.585362   \n",
       "2          -0.367745       -0.522967       0.522967         -0.585362   \n",
       "3          -0.367745       -0.522967       0.522967         -0.585362   \n",
       "4          -0.367745       -0.522967       0.522967         -0.585362   \n",
       "\n",
       "   destination_No Urgent Place  ...  RestaurantLessThan20_1~3  \\\n",
       "0                     1.009347  ...                  -0.85769   \n",
       "1                     1.009347  ...                  -0.85769   \n",
       "2                     1.009347  ...                  -0.85769   \n",
       "3                     1.009347  ...                  -0.85769   \n",
       "4                     1.009347  ...                  -0.85769   \n",
       "\n",
       "   RestaurantLessThan20_4~8  RestaurantLessThan20_gt8  \\\n",
       "0                  1.594684                 -0.335752   \n",
       "1                  1.594684                 -0.335752   \n",
       "2                  1.594684                 -0.335752   \n",
       "3                  1.594684                 -0.335752   \n",
       "4                  1.594684                 -0.335752   \n",
       "\n",
       "   RestaurantLessThan20_less1  RestaurantLessThan20_never  \\\n",
       "0                   -0.444545                   -0.132856   \n",
       "1                   -0.444545                   -0.132856   \n",
       "2                   -0.444545                   -0.132856   \n",
       "3                   -0.444545                   -0.132856   \n",
       "4                   -0.444545                   -0.132856   \n",
       "\n",
       "   Restaurant20To50_1~3  Restaurant20To50_4~8  Restaurant20To50_gt8  \\\n",
       "0              1.689769             -0.246759             -0.145795   \n",
       "1              1.689769             -0.246759             -0.145795   \n",
       "2              1.689769             -0.246759             -0.145795   \n",
       "3              1.689769             -0.246759             -0.145795   \n",
       "4              1.689769             -0.246759             -0.145795   \n",
       "\n",
       "   Restaurant20To50_less1  Restaurant20To50_never  \n",
       "0               -0.959053               -0.450003  \n",
       "1               -0.959053               -0.450003  \n",
       "2               -0.959053               -0.450003  \n",
       "3               -0.959053               -0.450003  \n",
       "4               -0.959053               -0.450003  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_coupons = pd.DataFrame(scaler.fit_transform(X_coupons), columns=X_coupons.columns)\n",
    "\n",
    "# Verify the scaling\n",
    "print(\"Scaled feature preview (first 5 rows):\")\n",
    "display(X_coupons.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee9a18-33e3-4f5f-a82a-d1ed68091c9c",
   "metadata": {},
   "source": [
    "# Running Models on Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9c5fd6b-3b3e-4922-8181-738cdcceaca0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import numpy as np\n",
    "\n",
    "# # 3 splits x 3 classifiers x 3 datasets x 3 trials\n",
    "# splits = {\n",
    "#     \"20/80\": 0.2,  # 20% test, 80% train\n",
    "#     \"50/50\": 0.5,  # 50% test, 50% train\n",
    "#     \"80/20\": 0.8   # 80% test, 20% train\n",
    "# }\n",
    "\n",
    "# classifiers = {\n",
    "#     \"Logistic Regression\": None,\n",
    "#     \"Random Forest\": None,\n",
    "#     \"Decision Tree\": None \n",
    "# }\n",
    "\n",
    "# datasets = {\n",
    "#     \"Occupancy Dataset\": (X_occupancy, y_occupancy),\n",
    "#     \"Sepsis Dataset\": (X_sepsis, y_sepsis),\n",
    "#     \"Coupon Dataset\": (X_coupons, y_coupons)\n",
    "# }\n",
    "\n",
    "# num_trials = 3\n",
    "# results = []\n",
    "\n",
    "# for dataset_name, (X, y) in datasets.items():\n",
    "#     print(f\"\\n--- Processing {dataset_name} ---\")\n",
    "    \n",
    "#     for split_name, test_size in splits.items():\n",
    "#         print(f\"\\n  Split: {split_name}\")\n",
    "        \n",
    "#         # Logistic Regression Experiment\n",
    "#         if \"Logistic Regression\" in classifiers:\n",
    "#             for regularization in [10**-8, 10**-6, 10**-4, 10**-2, 1, 10**2, 10**4]:\n",
    "#                 print(f\"    Classifier: Logistic Regression (C={regularization})\")\n",
    "                \n",
    "#                 log_model = LogisticRegression(max_iter=2000, solver=\"lbfgs\", C=regularization)\n",
    "                \n",
    "#                 train_accuracies = []\n",
    "#                 test_accuracies = []\n",
    "                \n",
    "#                 for trial in range(num_trials):\n",
    "#                     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#                         X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "#                     )\n",
    "\n",
    "#                     log_model.fit(X_train, y_train)\n",
    "\n",
    "#                     y_train_pred = log_model.predict(X_train)\n",
    "#                     y_test_pred = log_model.predict(X_test)\n",
    "\n",
    "#                     train_accuracies.append(accuracy_score(y_train, y_train_pred))\n",
    "#                     test_accuracies.append(accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "#                 avg_train = np.mean(train_accuracies)\n",
    "#                 avg_test = np.mean(test_accuracies)\n",
    "\n",
    "#                 print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "#                 print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "#                 results.append({\n",
    "#                     \"Dataset\": dataset_name,\n",
    "#                     \"Split\": split_name,\n",
    "#                     \"Classifier\": f\"Logistic Regression (C={regularization})\",\n",
    "#                     \"Train Accuracy\": avg_train,\n",
    "#                     \"Test Accuracy\": avg_test\n",
    "#                 })\n",
    "\n",
    "#         # Random Forest Experiment\n",
    "#         if \"Random Forest\" in classifiers:\n",
    "#             # Loop over different max_features values\n",
    "#             for max_features in [1, 2, 4, 6, 8, 12, 16, 20]:\n",
    "#                 print(f\"    Classifier: Random Forest (max_features={max_features})\")\n",
    "#                 random_forest_model = RandomForestClassifier(n_estimators=1024, max_features=max_features, random_state=42)\n",
    "\n",
    "#                 # Accumulate accuracies over trials\n",
    "#                 train_accuracies = []\n",
    "#                 val_accuracies = []\n",
    "#                 test_accuracies = []\n",
    "\n",
    "#                 for trial in range(num_trials):\n",
    "#                     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#                         X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "#                     )\n",
    "\n",
    "#                     # Fit the model on the training set\n",
    "#                     random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "#                     # Make predictions for train, validation, and test sets\n",
    "#                     y_train_pred = random_forest_model.predict(X_train)\n",
    "#                     y_test_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "#                     # Evaluate accuracies\n",
    "#                     train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "#                     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#                     # Store accuracies\n",
    "#                     train_accuracies.append(train_accuracy)\n",
    "#                     test_accuracies.append(test_accuracy)\n",
    "\n",
    "#                 # Compute average accuracies over trials\n",
    "#                 avg_train = np.mean(train_accuracies)\n",
    "#                 avg_test = np.mean(test_accuracies)\n",
    "\n",
    "#                 # Output results\n",
    "#                 print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "#                 print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "#                 # Store results\n",
    "#                 results.append({\n",
    "#                     \"Dataset\": dataset_name,\n",
    "#                     \"Split\": split_name,\n",
    "#                     \"Classifier\": f\"Random Forest (max_features={max_features})\",\n",
    "#                     \"Train Accuracy\": avg_train,\n",
    "#                     \"Test Accuracy\": avg_test\n",
    "#                 })\n",
    "\n",
    "#         # Decision Tree Experiment\n",
    "#         if \"Decision Tree\" in classifiers:\n",
    "#             for criterion in [\"gini\", \"entropy\"]:\n",
    "#                 print(f\"    Classifier: Decision Tree (Criterion={criterion})\")\n",
    "\n",
    "#                 train_accuracies = []\n",
    "#                 test_accuracies = []\n",
    "\n",
    "#                 for trial in range(num_trials):\n",
    "#                     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#                         X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "#                     )\n",
    "\n",
    "#                     dt_model = DecisionTreeClassifier(random_state=42, criterion=criterion)\n",
    "#                     dt_model.fit(X_train, y_train)\n",
    "\n",
    "#                     y_train_pred = dt_model.predict(X_train)\n",
    "#                     y_test_pred = dt_model.predict(X_test)\n",
    "\n",
    "#                     train_accuracies.append(accuracy_score(y_train, y_train_pred))\n",
    "#                     test_accuracies.append(accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "#                 avg_train = np.mean(train_accuracies)\n",
    "#                 avg_test = np.mean(test_accuracies)\n",
    "\n",
    "#                 print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "#                 print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "#                 results.append({\n",
    "#                     \"Dataset\": dataset_name,\n",
    "#                     \"Split\": split_name,\n",
    "#                     \"Classifier\": f\"Decision Tree (Criterion={criterion})\",\n",
    "#                     \"Train Accuracy\": avg_train,\n",
    "#                     \"Test Accuracy\": avg_test\n",
    "#                 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fae4b5de-60ed-484a-a965-0048d8bb47ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import numpy as np122\n",
    "\n",
    "# # 3 splits x 3 classifiers x 3 datasets x 3 trials\n",
    "# splits = {\n",
    "#     \"20/80\": 0.2,  # 20% test, 80% train\n",
    "#     \"50/50\": 0.5,  # 50% test, 50% train\n",
    "#     \"80/20\": 0.8   # 80% test, 20% train\n",
    "# }\n",
    "\n",
    "\n",
    "# classifiers = {\n",
    "#     # For now don't initialize the classifiers to anything so we can adjust the parameters according to that of the paper\n",
    "#     \"Logistic Regression\": None,  # \n",
    "#     # \"Random Forest\": None,  \n",
    "#     \"Decision Tree\": None \n",
    "# }\n",
    "\n",
    "# datasets = {\n",
    "#     \"Occupancy Dataset\": (X_occupancy, y_occupancy),\n",
    "#     \"Sepsis Dataset\": (X_sepsis, y_sepsis),\n",
    "#     \"Coupon Dataset\": (X_coupons, y_coupons)\n",
    "# }\n",
    "\n",
    "# num_trials = 3\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for dataset_name, (X, y) in datasets.items():\n",
    "#     print(f\"\\n--- Processing {dataset_name} ---\")\n",
    "    \n",
    "#     for split_name, test_size in splits.items():\n",
    "#         print(f\"\\n  Split: {split_name}\")\n",
    "        \n",
    "#         # Logistic Regression Experiment\n",
    "#         if \"Logistic Regression\" in classifiers:\n",
    "#             # Following papers different C values\n",
    "#             for regularization in [10**-8, 10**-6, 10**-4, 10**-2, 1, 10**2, 10**4]:\n",
    "#                 print(f\"    Classifier: Logistic Regression (C={regularization})\")\n",
    "                \n",
    "                \n",
    "#                 log_model = LogisticRegression(max_iter=2000, solver=\"lbfgs\", C=regularization)\n",
    "                \n",
    "#                 # Accumulate accuracies over trials so we can compute the average\n",
    "#                 train_accuracies = []\n",
    "#                 val_accuracies = []\n",
    "#                 test_accuracies = []\n",
    "                \n",
    "#                 for trial in range(num_trials):\n",
    "#                     # Training and Testing Splits, need stratify parameters to ensure equal splitting between trials\n",
    "#                     X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "#                         X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "#                     )\n",
    "\n",
    "#                     # validation split \n",
    "#                     X_train, X_val, y_train, y_val = train_test_split(\n",
    "#                         X_train_full, y_train_full, test_size=0.2, random_state=42 + trial, stratify=y_train_full\n",
    "#                     )\n",
    "\n",
    "#                     # Fit the model on the training set\n",
    "#                     log_model.fit(X_train, y_train)\n",
    "\n",
    "#                     # Make predictions for train, validation, and test sets\n",
    "#                     y_train_pred = log_model.predict(X_train)\n",
    "#                     y_val_pred = log_model.predict(X_val)\n",
    "#                     y_test_pred = log_model.predict(X_test)\n",
    "\n",
    "#                     # Evaluate accuracies\n",
    "#                     train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "#                     val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "#                     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "#                     # Store accuracies\n",
    "#                     train_accuracies.append(train_accuracy)\n",
    "#                     val_accuracies.append(val_accuracy)\n",
    "#                     test_accuracies.append(test_accuracy)\n",
    "\n",
    "#                 # Compute average accuracies over trials\n",
    "#                 avg_train = np.mean(train_accuracies)\n",
    "#                 avg_val_accuracy = np.mean(val_accuracies)\n",
    "#                 avg_test = np.mean(test_accuracies)\n",
    "\n",
    "#                 # Output results\n",
    "#                 print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "#                 print(f\"      Avg Validation Accuracy: {avg_val_accuracy:.4f}\")\n",
    "#                 print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "#                 # Store results\n",
    "#                 results.append({\n",
    "#                     \"Dataset\": dataset_name,\n",
    "#                     \"Split\": split_name,\n",
    "#                     \"Classifier\": f\"Logistic Regression (C={regularization})\",\n",
    "#                     \"Train Accuracy\": avg_train,\n",
    "#                     \"Validation Accuracy\": avg_val_accuracy,\n",
    "#                     \"Test Accuracy\": avg_test\n",
    "#                 })\n",
    "\n",
    "\n",
    "#         if \"Decision Tree\" in classifiers:\n",
    "#             for criterion in [\"gini\", \"entropy\"]: #testing the different splitting criterion according to paper\n",
    "#                 print(f\"    Classifier: Decision Tree (Criterion={criterion})\")\n",
    "        \n",
    "#                 train_accuracies = []\n",
    "#                 val_accuracies = []\n",
    "#                 test_accuracies = []\n",
    "        \n",
    "#                 for trial in range(num_trials):\n",
    "#                     X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "#                         X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "#                     )\n",
    "\n",
    "#                     X_train, X_val, y_train, y_val = train_test_split(\n",
    "#                         X_train_full, y_train_full, test_size=0.2, random_state=42 + trial, stratify=y_train_full\n",
    "#                     )\n",
    "        \n",
    "#                     dt_model = DecisionTreeClassifier(random_state=42, criterion=criterion)\n",
    "#                     dt_model.fit(X_train, y_train)\n",
    "        \n",
    "#                     y_train_pred = dt_model.predict(X_train)\n",
    "#                     y_val_pred = dt_model.predict(X_val)\n",
    "#                     y_test_pred = dt_model.predict(X_test)\n",
    "        \n",
    "#                     train_accuracies.append(accuracy_score(y_train, y_train_pred))\n",
    "#                     val_accuracies.append(accuracy_score(y_val, y_val_pred))\n",
    "#                     test_accuracies.append(accuracy_score(y_test, y_test_pred))\n",
    "        \n",
    "#                 avg_train = np.mean(train_accuracies)\n",
    "#                 avg_val_accuracy = np.mean(val_accuracies)\n",
    "#                 avg_test = np.mean(test_accuracies)\n",
    "        \n",
    "#                 print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "#                 print(f\"      Avg Validation Accuracy: {avg_val_accuracy:.4f}\")\n",
    "#                 print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "        \n",
    "#                 results.append({\n",
    "#                     \"Dataset\": dataset_name,\n",
    "#                     \"Split\": split_name,\n",
    "#                     \"Classifier\": f\"Decision Tree (Criterion={criterion})\",\n",
    "#                     \"Train Accuracy\": avg_train,\n",
    "#                     \"Validation Accuracy\": avg_val_accuracy,\n",
    "#                     \"Test Accuracy\": avg_test\n",
    "#                 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f074e225-dfe0-4c2c-b449-a5d81f05bf55",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import numpy as np\n",
    "\n",
    "# splits = {\n",
    "#     \"20/80\": 0.2,\n",
    "#     \"50/50\": 0.5,\n",
    "#     \"80/20\": 0.8\n",
    "# }\n",
    "\n",
    "# classifiers = {\n",
    "#     \"Logistic Regression\": LogisticRegression(max_iter=2000, solver=\"lbfgs\"),\n",
    "#     \"Random Forest\": RandomForestClassifier(n_estimators=1024, random_state=42),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(random_state=42)\n",
    "# }\n",
    "\n",
    "# datasets = {\n",
    "#     \"Occupancy Dataset\": (X_occupancy, y_occupancy),\n",
    "#     \"Sepsis Dataset\": (X_sepsis, y_sepsis),\n",
    "#     \"Coupon Dataset\": (X_coupons, y_coupons)\n",
    "# }\n",
    "\n",
    "# num_trials = 3\n",
    "# n_splits = 5  # For cross-validation\n",
    "# results = []\n",
    "\n",
    "# for dataset_name, (X, y) in datasets.items():\n",
    "#     print(f\"\\n--- Processing {dataset_name} ---\")\n",
    "\n",
    "#     for split_name, test_size in splits.items():\n",
    "#         print(f\"\\n  Split: {split_name}\")\n",
    "\n",
    "#         for classifier_name, classifier in classifiers.items():\n",
    "#             print(f\"    Classifier: {classifier_name}\")\n",
    "\n",
    "#             if classifier_name == \"Logistic Regression\":\n",
    "#                 hyper_params = {\"C\": [10**-8, 10**-6, 10**-4, 10**-2, 1, 10**2, 10**4]}\n",
    "#             elif classifier_name == \"Random Forest\":\n",
    "#                 hyper_params = {\"max_features\": [1, 2, 4, 6, 8, 12, 16, 20]}\n",
    "#             elif classifier_name == \"Decision Tree\":\n",
    "#                 hyper_params = {\"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "#             # Store best results for each classifier\n",
    "#             best_val_accuracy = -1\n",
    "#             best_params = None\n",
    "#             avg_train, avg_val_accuracy, avg_test = 0, 0, 0\n",
    "\n",
    "#             for param_name, param_values in hyper_params.items():\n",
    "#                 for param_value in param_values:\n",
    "#                     # Update hyper-parameter\n",
    "#                     classifier.set_params(**{param_name: param_value})\n",
    "\n",
    "#                     train_accuracies, val_accuracies, test_accuracies = [], [], []\n",
    "\n",
    "#                     for trial in range(num_trials):\n",
    "#                         X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "#                             X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "#                         )\n",
    "\n",
    "#                         X_train, X_val, y_train, y_val = train_test_split(\n",
    "#                             X_train_full, y_train_full, test_size=0.2, random_state=42 + trial, stratify=y_train_full\n",
    "#                         )\n",
    "\n",
    "#                         # Cross-validation\n",
    "#                         k_fold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42 + trial)\n",
    "#                         fold_train_acc, fold_val_acc = [], []\n",
    "\n",
    "#                         for train_index, val_index in k_fold.split(X_train, y_train):\n",
    "#                             X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "#                             y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "#                             classifier.fit(X_train_fold, y_train_fold)\n",
    "#                             fold_train_acc.append(accuracy_score(y_train_fold, classifier.predict(X_train_fold)))\n",
    "#                             fold_val_acc.append(accuracy_score(y_val_fold, classifier.predict(X_val_fold)))\n",
    "\n",
    "#                         # Use average validation accuracy across folds\n",
    "#                         train_accuracy = np.mean(fold_train_acc)\n",
    "#                         val_accuracy = np.mean(fold_val_acc)\n",
    "\n",
    "#                         # Evaluate on held-out test set\n",
    "#                         classifier.fit(X_train, y_train)\n",
    "#                         test_accuracy = accuracy_score(y_test, classifier.predict(X_test))\n",
    "\n",
    "#                         train_accuracies.append(train_accuracy)\n",
    "#                         val_accuracies.append(val_accuracy)\n",
    "#                         test_accuracies.append(test_accuracy)\n",
    "\n",
    "#                     # Average over trials\n",
    "#                     avg_train = np.mean(train_accuracies)\n",
    "#                     avg_val_accuracy = np.mean(val_accuracies)\n",
    "#                     avg_test = np.mean(test_accuracies)\n",
    "\n",
    "#                     if avg_val_accuracy > best_val_accuracy:\n",
    "#                         best_val_accuracy = avg_val_accuracy\n",
    "#                         best_params = {param_name: param_value}\n",
    "\n",
    "#             print(f\"      Best Params: {best_params}\")\n",
    "#             print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "#             print(f\"      Avg Validation Accuracy: {avg_val_accuracy:.4f}\")\n",
    "#             print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "#             results.append({\n",
    "#                 \"Dataset\": dataset_name,\n",
    "#                 \"Split\": split_name,\n",
    "#                 \"Classifier\": classifier_name,\n",
    "#                 \"Best Params\": best_params,\n",
    "#                 \"Train Accuracy\": avg_train,\n",
    "#                 \"Validation Accuracy\": avg_val_accuracy,\n",
    "#                 \"Test Accuracy\": avg_test\n",
    "#             })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43abc42e-f054-4c41-8d97-3bdaba9b038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define datasets and splits\n",
    "# datasets = {\n",
    "#     \"Occupancy Dataset\": (X_occupancy, y_occupancy),\n",
    "#     \"Sepsis Dataset\": (X_sepsis, y_sepsis),\n",
    "#     \"Coupon Dataset\": (X_coupons, y_coupons)\n",
    "# }\n",
    "# splits = {\"20/80\": 0.2, \"50/50\": 0.5, \"80/20\": 0.8}\n",
    "\n",
    "# # Hyperparameter grids\n",
    "# hyperparams = {\n",
    "#     \"Logistic Regression\": {\n",
    "#         \"C\": [10**-8, 10**-6, 10**-4, 10**-2, 1, 10**2, 10**4],\n",
    "#         \"solver\": [\"lbfgs\"],\n",
    "#         \"max_iter\": [2000]\n",
    "#     },\n",
    "#     # \"Random Forest\": {\n",
    "#     #     \"n_estimators\": [1024],\n",
    "#     #     \"max_features\": [1, 2, 4, 6, 8, 12, 16, 20]\n",
    "#     # },\n",
    "#     \"Decision Tree\": {\n",
    "#         \"criterion\": [\"gini\", \"entropy\"]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Initialize results list\n",
    "# results = []\n",
    "\n",
    "# for dataset_name, (X, y) in datasets.items():\n",
    "#     print(f\"\\n--- Processing {dataset_name} ---\")\n",
    "\n",
    "#     for split_name, test_size in splits.items():\n",
    "#         print(f\"  Split: {split_name}\")\n",
    "\n",
    "#         # Split the dataset into training and testing sets\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             X, y.values.ravel(), test_size=test_size, random_state=42, stratify=y\n",
    "#         )\n",
    "\n",
    "#         # Loop through classifiers\n",
    "#         for clf_name, param_grid in hyperparams.items():\n",
    "#             print(f\"    Classifier: {clf_name}\")\n",
    "\n",
    "#             # Initialize the model\n",
    "#             if clf_name == \"Logistic Regression\":\n",
    "#                 model = LogisticRegression()\n",
    "#             elif clf_name == \"Random Forest\":\n",
    "#                 model = RandomForestClassifier(random_state=42)\n",
    "#             elif clf_name == \"Decision Tree\":\n",
    "#                 model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "#             # Use GridSearchCV function to perform 5-fold cross validation as stated by the paper, finds the optimal hyper parameters\n",
    "#             grid_search = GridSearchCV(model, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "#             grid_search.fit(X_train, y_train)\n",
    "\n",
    "#             # Get the best model and parameters\n",
    "#             best_model = grid_search.best_estimator_\n",
    "#             best_params = grid_search.best_params_\n",
    "#             print(f\"      Best Hyperparameters: {best_params}\")\n",
    "\n",
    "#             # Evaluate the model on the test set\n",
    "#             y_test_pred = best_model.predict(X_test)\n",
    "#             test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#             print(f\"      Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#             # Save results\n",
    "#             results.append({\n",
    "#                 \"Dataset\": dataset_name,\n",
    "#                 \"Split\": split_name,\n",
    "#                 \"Classifier\": clf_name,\n",
    "#                 \"Best Hyperparameters\": best_params,\n",
    "#                 \"Test Accuracy\": test_accuracy\n",
    "#             })\n",
    "\n",
    "# # Convert results to a DataFrame and display\n",
    "# results_df = pd.DataFrame(results)\n",
    "# print(\"\\n--- Cross-Validation Results ---\")\n",
    "# print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d664ba41-ce07-46fc-871b-7ff9e53af3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Occupancy Dataset\n",
      "\n",
      "20/80\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.8426\n",
      "      Avg Cross-Validation Accuracy: 0.8155\n",
      "      Avg Test Accuracy: 0.8436\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.9894\n",
      "      Avg Cross-Validation Accuracy: 0.9894\n",
      "      Avg Test Accuracy: 0.9886\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.9893\n",
      "      Avg Cross-Validation Accuracy: 0.9893\n",
      "      Avg Test Accuracy: 0.9883\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.9893\n",
      "      Avg Cross-Validation Accuracy: 0.9893\n",
      "      Avg Test Accuracy: 0.9884\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.9893\n",
      "      Avg Cross-Validation Accuracy: 0.9893\n",
      "      Avg Test Accuracy: 0.9883\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=0.01\n",
      " Optimal Cross Validation Accuracy0.9894\n",
      "    Classifier: Random Forest (max_features=1)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9938\n",
      "      Avg Test Accuracy: 0.9938\n",
      "    Classifier: Random Forest (max_features=2)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9937\n",
      "      Avg Test Accuracy: 0.9936\n",
      "    Classifier: Random Forest (max_features=4)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9936\n",
      "      Avg Test Accuracy: 0.9937\n",
      "    Classifier: Random Forest (max_features=6)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9933\n",
      "      Avg Test Accuracy: 0.9938\n",
      "    Classifier: Random Forest (max_features=8)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9932\n",
      "      Avg Test Accuracy: 0.9937\n",
      "    Classifier: Random Forest (max_features=12)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9932\n",
      "      Avg Test Accuracy: 0.9937\n",
      "    Classifier: Random Forest (max_features=16)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9932\n",
      "      Avg Test Accuracy: 0.9937\n",
      "    Classifier: Random Forest (max_features=20)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9932\n",
      "      Avg Test Accuracy: 0.9937\n",
      "  Optimal Max Features for Random Forest: max_features=1, \n",
      " Optimal Cross Validation Accuracy: 0.9938\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9917\n",
      "      Avg Test Accuracy: 0.9933\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9926\n",
      "      Avg Test Accuracy: 0.9929\n",
      "  Optimal Criterion for Decision Tree: Criterion=entropy\n",
      " Optimal Cross Validation Accuracy: 0.9926\n",
      "\n",
      "50/50\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.7865\n",
      "      Avg Cross-Validation Accuracy: 0.7716\n",
      "      Avg Test Accuracy: 0.7873\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.9895\n",
      "      Avg Cross-Validation Accuracy: 0.9895\n",
      "      Avg Test Accuracy: 0.9889\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.9895\n",
      "      Avg Cross-Validation Accuracy: 0.9894\n",
      "      Avg Test Accuracy: 0.9888\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.9895\n",
      "      Avg Cross-Validation Accuracy: 0.9895\n",
      "      Avg Test Accuracy: 0.9888\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.9894\n",
      "      Avg Cross-Validation Accuracy: 0.9895\n",
      "      Avg Test Accuracy: 0.9888\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=0.01\n",
      " Optimal Cross Validation Accuracy0.9895\n",
      "    Classifier: Random Forest (max_features=1)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9928\n",
      "      Avg Test Accuracy: 0.9931\n",
      "    Classifier: Random Forest (max_features=2)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9926\n",
      "      Avg Test Accuracy: 0.9931\n",
      "    Classifier: Random Forest (max_features=4)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9925\n",
      "      Avg Test Accuracy: 0.9930\n",
      "    Classifier: Random Forest (max_features=6)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9923\n",
      "      Avg Test Accuracy: 0.9930\n",
      "    Classifier: Random Forest (max_features=8)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9922\n",
      "      Avg Test Accuracy: 0.9928\n",
      "    Classifier: Random Forest (max_features=12)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9922\n",
      "      Avg Test Accuracy: 0.9928\n",
      "    Classifier: Random Forest (max_features=16)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9922\n",
      "      Avg Test Accuracy: 0.9928\n",
      "    Classifier: Random Forest (max_features=20)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9922\n",
      "      Avg Test Accuracy: 0.9928\n",
      "  Optimal Max Features for Random Forest: max_features=1, \n",
      " Optimal Cross Validation Accuracy: 0.9928\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9902\n",
      "      Avg Test Accuracy: 0.9909\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9911\n",
      "      Avg Test Accuracy: 0.9921\n",
      "  Optimal Criterion for Decision Tree: Criterion=entropy\n",
      " Optimal Cross Validation Accuracy: 0.9911\n",
      "\n",
      "80/20\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7691\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.9896\n",
      "      Avg Cross-Validation Accuracy: 0.9887\n",
      "      Avg Test Accuracy: 0.9872\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.9910\n",
      "      Avg Cross-Validation Accuracy: 0.9908\n",
      "      Avg Test Accuracy: 0.9886\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.9907\n",
      "      Avg Cross-Validation Accuracy: 0.9907\n",
      "      Avg Test Accuracy: 0.9887\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.9907\n",
      "      Avg Cross-Validation Accuracy: 0.9908\n",
      "      Avg Test Accuracy: 0.9887\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=1\n",
      " Optimal Cross Validation Accuracy0.9908\n",
      "    Classifier: Random Forest (max_features=1)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9932\n",
      "      Avg Test Accuracy: 0.9915\n",
      "    Classifier: Random Forest (max_features=2)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9925\n",
      "      Avg Test Accuracy: 0.9912\n",
      "    Classifier: Random Forest (max_features=4)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9924\n",
      "      Avg Test Accuracy: 0.9910\n",
      "    Classifier: Random Forest (max_features=6)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9926\n",
      "      Avg Test Accuracy: 0.9910\n",
      "    Classifier: Random Forest (max_features=8)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9925\n",
      "      Avg Test Accuracy: 0.9906\n",
      "    Classifier: Random Forest (max_features=12)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9925\n",
      "      Avg Test Accuracy: 0.9906\n",
      "    Classifier: Random Forest (max_features=16)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9925\n",
      "      Avg Test Accuracy: 0.9906\n",
      "    Classifier: Random Forest (max_features=20)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9925\n",
      "      Avg Test Accuracy: 0.9906\n",
      "  Optimal Max Features for Random Forest: max_features=1, \n",
      " Optimal Cross Validation Accuracy: 0.9932\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9898\n",
      "      Avg Test Accuracy: 0.9880\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9906\n",
      "      Avg Test Accuracy: 0.9884\n",
      "  Optimal Criterion for Decision Tree: Criterion=entropy\n",
      " Optimal Cross Validation Accuracy: 0.9906\n",
      "\n",
      "Sepsis Dataset\n",
      "\n",
      "20/80\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=1e-08\n",
      " Optimal Cross Validation Accuracy0.9263\n",
      "    Classifier: Random Forest (max_features=1)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9261\n",
      "      Avg Test Accuracy: 0.9262\n",
      "    Classifier: Random Forest (max_features=2)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9262\n",
      "      Avg Test Accuracy: 0.9262\n",
      "    Classifier: Random Forest (max_features=4)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9261\n",
      "      Avg Test Accuracy: 0.9262\n",
      "    Classifier: Random Forest (max_features=6)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9261\n",
      "      Avg Test Accuracy: 0.9262\n",
      "    Classifier: Random Forest (max_features=8)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9261\n",
      "      Avg Test Accuracy: 0.9262\n",
      "    Classifier: Random Forest (max_features=12)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9261\n",
      "      Avg Test Accuracy: 0.9262\n",
      "    Classifier: Random Forest (max_features=16)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9261\n",
      "      Avg Test Accuracy: 0.9262\n",
      "    Classifier: Random Forest (max_features=20)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9261\n",
      "      Avg Test Accuracy: 0.9262\n",
      "  Optimal Max Features for Random Forest: max_features=2, \n",
      " Optimal Cross Validation Accuracy: 0.9262\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9260\n",
      "      Avg Test Accuracy: 0.9259\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9260\n",
      "      Avg Test Accuracy: 0.9259\n",
      "  Optimal Criterion for Decision Tree: Criterion=gini\n",
      " Optimal Cross Validation Accuracy: 0.9260\n",
      "\n",
      "50/50\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=1e-08\n",
      " Optimal Cross Validation Accuracy0.9263\n",
      "    Classifier: Random Forest (max_features=1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 130\u001b[0m\n\u001b[1;32m    125\u001b[0m X_train_full, X_test, y_train_full, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m    126\u001b[0m     X, y\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel(), test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m \u001b[38;5;241m+\u001b[39m trial, stratify\u001b[38;5;241m=\u001b[39my\n\u001b[1;32m    127\u001b[0m )\n\u001b[1;32m    129\u001b[0m k_fold \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m \u001b[38;5;241m+\u001b[39m trial)\n\u001b[0;32m--> 130\u001b[0m cross_val_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_forest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m avg_cross_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(cross_val_results)\n\u001b[1;32m    132\u001b[0m trial_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross Validation Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(avg_cross_val)\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:910\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    907\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    909\u001b[0m fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m--> 910\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m \u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_params_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m score_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time \u001b[38;5;241m-\u001b[39m fit_time\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:971\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, score_params, error_score)\u001b[0m\n\u001b[1;32m    969\u001b[0m         scores \u001b[38;5;241m=\u001b[39m scorer(estimator, X_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscore_params)\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscore_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[1;32m    974\u001b[0m         \u001b[38;5;66;03m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[1;32m    975\u001b[0m         \u001b[38;5;66;03m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:139\u001b[0m, in \u001b[0;36m_MultimetricScorer.__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer, _BaseScorer):\n\u001b[0;32m--> 139\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcached_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m         score \u001b[38;5;241m=\u001b[39m scorer(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mget(name)\u001b[38;5;241m.\u001b[39mscore)\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:371\u001b[0m, in \u001b[0;36m_Scorer._score\u001b[0;34m(self, method_caller, estimator, X, y_true, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m pos_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m is_regressor(estimator) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pos_label()\n\u001b[1;32m    370\u001b[0m response_method \u001b[38;5;241m=\u001b[39m _check_response_method(estimator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_method)\n\u001b[0;32m--> 371\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmethod_caller\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m scoring_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sign \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_score_func(y_true, y_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscoring_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/metrics/_scorer.py:89\u001b[0m, in \u001b[0;36m_cached_call\u001b[0;34m(cache, estimator, response_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response_method \u001b[38;5;129;01min\u001b[39;00m cache:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cache[response_method]\n\u001b[0;32m---> 89\u001b[0m result, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_get_response_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     cache[response_method] \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/utils/_response.py:211\u001b[0m, in \u001b[0;36m_get_response_values\u001b[0;34m(estimator, X, response_method, pos_label, return_response_method_used)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m target_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    209\u001b[0m         pos_label \u001b[38;5;241m=\u001b[39m classes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 211\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_log_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    214\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m _process_predict_proba(\n\u001b[1;32m    215\u001b[0m         y_pred\u001b[38;5;241m=\u001b[39my_pred,\n\u001b[1;32m    216\u001b[0m         target_type\u001b[38;5;241m=\u001b[39mtarget_type,\n\u001b[1;32m    217\u001b[0m         classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[1;32m    218\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[1;32m    219\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:904\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    884\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 904\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:957\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    952\u001b[0m all_proba \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    953\u001b[0m     np\u001b[38;5;241m.\u001b[39mzeros((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], j), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[1;32m    955\u001b[0m ]\n\u001b[1;32m    956\u001b[0m lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[0;32m--> 957\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msharedmem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_accumulate_prediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_proba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proba \u001b[38;5;129;01min\u001b[39;00m all_proba:\n\u001b[1;32m    963\u001b[0m     proba \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_)\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:731\u001b[0m, in \u001b[0;36m_accumulate_prediction\u001b[0;34m(predict, X, out, lock)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_accumulate_prediction\u001b[39m(predict, X, out, lock):\n\u001b[1;32m    725\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;124;03m    This is a utility function for joblib's Parallel.\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m    It can't go locally in ForestClassifier or ForestRegressor, because joblib\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;124;03m    complains that it cannot pickle it when placed there.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 731\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cogs118a/lib/python3.12/site-packages/sklearn/tree/_classes.py:1043\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict_proba\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m   1041\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1042\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X, check_input)\n\u001b[0;32m-> 1043\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proba[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 3 splits x 3 classifiers x 3 datasets x 3 trials\n",
    "splits = {\n",
    "    \"20/80\": 0.2,  # 20% test, 80% train\n",
    "    \"50/50\": 0.5,  # 50% test, 50% train\n",
    "    \"80/20\": 0.8   # 80% test, 20% train\n",
    "}\n",
    "\n",
    "#To be later initialized so can tune hyper parameters\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": None,\n",
    "    \"Random Forest\": None,\n",
    "    \"Decision Tree\": None\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"Occupancy Dataset\": (X_occupancy, y_occupancy),\n",
    "    \"Sepsis Dataset\": (X_sepsis, y_sepsis),\n",
    "    \"Coupon Dataset\": (X_coupons, y_coupons)\n",
    "}\n",
    "\n",
    "num_trials = 3\n",
    "\n",
    "#results store values for later use when creating heatmap\n",
    "results = []\n",
    "\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "    \n",
    "    for split_name, test_size in splits.items():\n",
    "        print(f\"\\n{split_name}\")\n",
    "        \n",
    "        if \"Logistic Regression\" in classifiers:\n",
    "            optimal_log_param = None\n",
    "            opt_log_cross_val = -1  # dummy value place holder\n",
    "\n",
    "            #iterates through all possible regularization parameters\n",
    "            for regularization in [10**-8, 10**-6, 10**-4, 10**-2, 1, 10**2, 10**4]:\n",
    "                print(f\"Logistic Regression (C={regularization})\")\n",
    "                \n",
    "                log_model = LogisticRegression(max_iter=2000, solver=\"lbfgs\", C=regularization)\n",
    "\n",
    "     \n",
    "                trial_results = {\n",
    "                    \"Train Accuracy\": [],\n",
    "                    \"Cross Validation Accuracy\": [],\n",
    "                    \"Test Accuracy\": []\n",
    "                }\n",
    "\n",
    "                #Iterates for 3 trials as indicated\n",
    "                for trial in range(num_trials):\n",
    "                    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "                        X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "                    )\n",
    "                    \n",
    "                    # 5-fold cross validation\n",
    "                    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "                    cross_val_results = cross_val_score(log_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "                    avg_cross_val = np.mean(cross_val_results)\n",
    "                    trial_results[\"Cross Validation Accuracy\"].append(avg_cross_val)\n",
    "\n",
    "                    #training the model\n",
    "                    log_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "                    y_train_full_pred = log_model.predict(X_train_full)\n",
    "                    train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "                    trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "\n",
    "                    #evaluating tet\n",
    "                    y_test_pred = log_model.predict(X_test)\n",
    "                    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "                    trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "\n",
    "                # taking the average over the 3 trials\n",
    "                avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "                avg_cross_val = np.mean(trial_results[\"Cross Validation Accuracy\"])\n",
    "                avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "\n",
    "                # select optimal parameter \n",
    "                if avg_cross_val > opt_log_cross_val:\n",
    "                    opt_log_cross_val = avg_cross_val\n",
    "                    optimal_log_param = regularization\n",
    "\n",
    "                # Output results\n",
    "                print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "                print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "                print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "                results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Split\": split_name,\n",
    "                    \"Classifier\": f\"Logistic Regression (C={regularization})\",\n",
    "                    \"Train Accuracy\": avg_train,\n",
    "                    \"Cross Validation Accuracy\": avg_cross_val,\n",
    "                    \"Test Accuracy\": avg_test\n",
    "                })\n",
    "\n",
    "            print(f\"  Optimal Regularization Parameter for Logistic Regression: C={optimal_log_param}\\n Optimal Cross Validation Accuracy{opt_log_cross_val:.4f}\")\n",
    "\n",
    "        if \"Random Forest\" in classifiers:\n",
    "            optimal_max_feat = None\n",
    "            opt_random_forest_cross_val = -1  \n",
    "            \n",
    "            # Iterate over different max_features values\n",
    "            for max_features in [1, 2, 4, 6, 8, 12, 16, 20]:\n",
    "                print(f\"    Classifier: Random Forest (max_features={max_features})\")\n",
    "                random_forest_model = RandomForestClassifier(n_estimators=1024, max_features=max_features, random_state=42)\n",
    "        \n",
    "                trial_results = {\n",
    "                    \"Train Accuracy\": [],\n",
    "                    \"Cross Validation Accuracy\": [],\n",
    "                    \"Test Accuracy\": []\n",
    "                }\n",
    "        \n",
    "                for trial in range(num_trials):\n",
    "                    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "                        X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "                    )\n",
    "        \n",
    "                    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "                    cross_val_results = cross_val_score(random_forest_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "                    avg_cross_val = np.mean(cross_val_results)\n",
    "                    trial_results[\"Cross Validation Accuracy\"].append(avg_cross_val)\n",
    "        \n",
    "                    random_forest_model.fit(X_train_full, y_train_full)\n",
    "        \n",
    "                    y_train_full_pred = random_forest_model.predict(X_train_full)\n",
    "                    train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "                    trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "        \n",
    "                    y_test_pred = random_forest_model.predict(X_test)\n",
    "                    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "                    trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "        \n",
    "                avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "                avg_cross_val = np.mean(trial_results[\"Cross Validation Accuracy\"])\n",
    "                avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "        \n",
    "\n",
    "                if avg_cross_val > opt_random_forest_cross_val:\n",
    "                    opt_random_forest_cross_val = avg_cross_val\n",
    "                    optimal_max_feat = max_features\n",
    "        \n",
    "                print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "                print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "                print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "        \n",
    "                results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Split\": split_name,\n",
    "                    \"Classifier\": f\"Random Forest (max_features={max_features})\",\n",
    "                    \"Train Accuracy\": avg_train,\n",
    "                    \"Cross Validation Accuracy\": avg_cross_val,\n",
    "                    \"Test Accuracy\": avg_test\n",
    "                })\n",
    "        \n",
    "            print(f\"  Optimal Max Features for Random Forest: max_features={optimal_max_feat}, \\n Optimal Cross Validation Accuracy: {opt_random_forest_cross_val:.4f}\")\n",
    "\n",
    "        if \"Decision Tree\" in classifiers:\n",
    "            opt_criterion = None\n",
    "            opt_decision_cross_val = -1\n",
    "            \n",
    "            for criterion in [\"gini\", \"entropy\"]:\n",
    "                print(f\"    Classifier: Decision Tree (Criterion={criterion})\")\n",
    "                \n",
    "                dt_model = DecisionTreeClassifier(random_state=42, criterion=criterion)\n",
    "\n",
    "                trial_results = {\n",
    "                    \"Train Accuracy\": [],\n",
    "                    \"Cross Validation Accuracy\": [],\n",
    "                    \"Test Accuracy\": []\n",
    "                }\n",
    "\n",
    "                for trial in range(num_trials):\n",
    "                    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "                        X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "                    )\n",
    "\n",
    "                    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "                    cross_val_results = cross_val_score(dt_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "                    avg_cross_val = np.mean(cross_val_results)\n",
    "                    trial_results[\"Cross Validation Accuracy\"].append(avg_cross_val)\n",
    "\n",
    "                    dt_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "                    y_train_full_pred = dt_model.predict(X_train_full)\n",
    "                    train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "                    trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "\n",
    "                    y_test_pred = dt_model.predict(X_test)\n",
    "                    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "                    trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "\n",
    "                avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "                avg_cross_val = np.mean(trial_results[\"Cross Validation Accuracy\"])\n",
    "                avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "\n",
    "                if avg_cross_val > opt_decision_cross_val:\n",
    "                    opt_decision_cross_val = avg_cross_val\n",
    "                    opt_criterion = criterion\n",
    "\n",
    "                print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "                print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "                print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Split\": split_name,\n",
    "                    \"Classifier\": f\"Decision Tree (Criterion={criterion})\",\n",
    "                    \"Train Accuracy\": avg_train,\n",
    "                    \"Cross Validation Accuracy\": avg_cross_val,\n",
    "                    \"Test Accuracy\": avg_test\n",
    "                })\n",
    "\n",
    "            print(f\"  Optimal Criterion for Decision Tree: Criterion={opt_criterion}\\n Optimal Cross Validation Accuracy: {opt_decision_cross_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0b2756-873b-408d-8e5f-8cef377d529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert results list to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create a heatmap for each classifier showing accuracies\n",
    "classifiers = results_df[\"Classifier\"].unique()\n",
    "\n",
    "# Loop over each classifier\n",
    "for clf_name in classifiers:\n",
    "    print(f\"--- Heatmap for {clf_name} ---\")\n",
    "    \n",
    "    # Filter the results for the current classifier\n",
    "    clf_results = results_df[results_df[\"Classifier\"] == clf_name]\n",
    "    \n",
    "    # Create a pivot table where rows are datasets, columns are splits, and values are accuracies\n",
    "    pivot_table = clf_results.pivot_table(\n",
    "        index=\"Dataset\", \n",
    "        columns=\"Split\", \n",
    "        values=\"Test Accuracy\", \n",
    "        aggfunc=\"mean\"  # Taking the mean in case there are multiple entries\n",
    "    )\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pivot_table, annot=True, cmap=\"Blues\", cbar=True, fmt=\".4f\", linewidths=0.5)\n",
    "    plt.title(f\"Test Accuracy Heatmap for {clf_name}\")\n",
    "    plt.ylabel(\"Dataset\")\n",
    "    plt.xlabel(\"Split\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222fe918-51f2-446e-8c3e-07f89c6b95c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import numpy as np\n",
    "\n",
    "# # 3 splits x 3 classifiers x 3 datasets x 3 trials\n",
    "# splits = {\n",
    "#     \"20/80\": 0.2,  # 20% test, 80% train\n",
    "#     \"50/50\": 0.5,  # 50% test, 50% train\n",
    "#     \"80/20\": 0.8   # 80% test, 20% train\n",
    "# }\n",
    "\n",
    "# classifiers = {\n",
    "#     \"Logistic Regression\": None,\n",
    "#     \"Random Forest\": None,\n",
    "#     \"Decision Tree\": None\n",
    "# }\n",
    "\n",
    "# datasets = {\n",
    "#     \"Occupancy Dataset\": (X_occupancy, y_occupancy),\n",
    "#     \"Sepsis Dataset\": (X_sepsis, y_sepsis),\n",
    "#     \"Coupon Dataset\": (X_coupons, y_coupons)\n",
    "# }\n",
    "\n",
    "# num_trials = 3\n",
    "\n",
    "# #results store values for later use when creating heatmap\n",
    "# results = []\n",
    "\n",
    "# for dataset_name, (X, y) in datasets.items():\n",
    "#     print(f\"\\n--- Processing {dataset_name} ---\")\n",
    "    \n",
    "#     for split_name, test_size in splits.items():\n",
    "#         print(f\"\\n  Split: {split_name}\")\n",
    "        \n",
    "#         # Logistic Regression Experiment\n",
    "#         if \"Logistic Regression\" in classifiers:\n",
    "#             optimal_log_param = None\n",
    "#             opt_log_cross_val = -1  #dummy val\n",
    "            \n",
    "#             for regularization in [10**-8, 10**-6, 10**-4, 10**-2, 1, 10**2, 10**4]:\n",
    "#                 print(f\"    Classifier: Logistic Regression (C={regularization})\")\n",
    "                \n",
    "#                 log_model = LogisticRegression(max_iter=2000, solver=\"lbfgs\", C=regularization)\n",
    "\n",
    "#                 # Accumulate results over trials\n",
    "#                 trial_results = {\n",
    "#                     \"Train Accuracy\": [],\n",
    "#                     \"CV Accuracy\": [],\n",
    "#                     \"Test Accuracy\": []\n",
    "#                 }\n",
    "\n",
    "#                 for trial in range(num_trials):\n",
    "#                     # Train-test split for the partition\n",
    "#                     X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "#                         X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "#                     )\n",
    "                    \n",
    "#                     # StratifiedKFold for 5-fold cross-validation\n",
    "#                     k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "#                     cross_val_results = cross_val_score(log_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "#                     avg_cross_val = np.mean(cross_val_results)\n",
    "#                     trial_results[\"CV Accuracy\"].append(avg_cross_val)\n",
    "\n",
    "#                     # Fit the model on the entire training set\n",
    "#                     log_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "#                     # Evaluate on the training set\n",
    "#                     y_train_full_pred = log_model.predict(X_train_full)\n",
    "#                     train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "#                     trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "\n",
    "#                     # Evaluate on the test set\n",
    "#                     y_test_pred = log_model.predict(X_test)\n",
    "#                     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#                     trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "\n",
    "#                 # Compute average accuracies over trials\n",
    "#                 avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "#                 avg_cross_val = np.mean(trial_results[\"CV Accuracy\"])\n",
    "#                 avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "\n",
    "#                 # Update the best hyperparameter if this C results in better CV accuracy\n",
    "#                 if avg_cross_val > opt_log_cross_val:\n",
    "#                     opt_log_cross_val = avg_cross_val\n",
    "#                     optimal_log_param = regularization\n",
    "\n",
    "#                 # Output results\n",
    "#                 print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "#                 print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "#                 print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "#                 # Store results\n",
    "#                 results.append({\n",
    "#                     \"Dataset\": dataset_name,\n",
    "#                     \"Split\": split_name,\n",
    "#                     \"Classifier\": f\"Logistic Regression (C={regularization})\",\n",
    "#                     \"Train Accuracy\": avg_train,\n",
    "#                     \"CV Accuracy\": avg_cross_val,\n",
    "#                     \"Test Accuracy\": avg_test\n",
    "#                 })\n",
    "\n",
    "#             # After the trials, print the optimal hyperparameter for Logistic Regression\n",
    "#             print(f\"  Optimal Hyperparameter for Logistic Regression: C={optimal_log_param} with CV Accuracy={opt_log_cross_val:.4f}\")\n",
    "\n",
    "#         # Random Forest Experiment\n",
    "#         if \"Random Forest\" in classifiers:\n",
    "#             optimal_max_feat = None\n",
    "#             opt_random_forest_cross_val = -np.inf  # Initialize with very low value\n",
    "            \n",
    "#             # Loop over different max_features values\n",
    "#             for max_features in [1, 2, 4, 6, 8, 12, 16, 20]:\n",
    "#                 print(f\"    Classifier: Random Forest (max_features={max_features})\")\n",
    "#                 random_forest_model = RandomForestClassifier(n_estimators=1024, max_features=max_features, random_state=42)\n",
    "        \n",
    "#                 # Accumulate results over trials\n",
    "#                 trial_results = {\n",
    "#                     \"Train Accuracy\": [],\n",
    "#                     \"CV Accuracy\": [],\n",
    "#                     \"Test Accuracy\": []\n",
    "#                 }\n",
    "        \n",
    "#                 for trial in range(num_trials):\n",
    "#                     # Train-test split for the partition\n",
    "#                     X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "#                         X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "#                     )\n",
    "        \n",
    "#                     # StratifiedKFold for 5-fold cross-validation\n",
    "#                     k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "#                     cross_val_results = cross_val_score(random_forest_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "#                     avg_cross_val = np.mean(cross_val_results)\n",
    "#                     trial_results[\"CV Accuracy\"].append(avg_cross_val)\n",
    "        \n",
    "#                     # Fit the model on the entire training set\n",
    "#                     random_forest_model.fit(X_train_full, y_train_full)\n",
    "        \n",
    "#                     # Evaluate on the training set\n",
    "#                     y_train_full_pred = random_forest_model.predict(X_train_full)\n",
    "#                     train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "#                     trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "        \n",
    "#                     # Evaluate on the test set\n",
    "#                     y_test_pred = random_forest_model.predict(X_test)\n",
    "#                     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#                     trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "        \n",
    "#                 # Compute average accuracies over trials\n",
    "#                 avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "#                 avg_cross_val = np.mean(trial_results[\"CV Accuracy\"])\n",
    "#                 avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "        \n",
    "#                 # Update the best hyperparameter if this max_features results in better CV accuracy\n",
    "#                 if avg_cross_val > opt_random_forest_cross_val:\n",
    "#                     opt_random_forest_cross_val = avg_cross_val\n",
    "#                     optimal_max_feat = max_features\n",
    "        \n",
    "#                 # Output results\n",
    "#                 print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "#                 print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "#                 print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "        \n",
    "#                 # Store results\n",
    "#                 results.append({\n",
    "#                     \"Dataset\": dataset_name,\n",
    "#                     \"Split\": split_name,\n",
    "#                     \"Classifier\": f\"Random Forest (max_features={max_features})\",\n",
    "#                     \"Train Accuracy\": avg_train,\n",
    "#                     \"CV Accuracy\": avg_cross_val,\n",
    "#                     \"Test Accuracy\": avg_test\n",
    "#                 })\n",
    "        \n",
    "#             # After the trials, print the optimal hyperparameter for Random Forest\n",
    "#             print(f\"  Optimal Hyperparameter for Random Forest: max_features={optimal_max_feat} with CV Accuracy={opt_random_forest_cross_val:.4f}\")\n",
    "    \n",
    "#         # Decision Tree Experiment\n",
    "#         if \"Decision Tree\" in classifiers:\n",
    "#             opt_criterion = None\n",
    "#             opt_decision_cross_val = -np.inf  # Initialize with very low value\n",
    "            \n",
    "#             for criterion in [\"gini\", \"entropy\"]:\n",
    "#                 print(f\"    Classifier: Decision Tree (Criterion={criterion})\")\n",
    "                \n",
    "#                 dt_model = DecisionTreeClassifier(random_state=42, criterion=criterion)\n",
    "\n",
    "#                 # Accumulate results over trials\n",
    "#                 trial_results = {\n",
    "#                     \"Train Accuracy\": [],\n",
    "#                     \"CV Accuracy\": [],\n",
    "#                     \"Test Accuracy\": []\n",
    "#                 }\n",
    "\n",
    "#                 for trial in range(num_trials):\n",
    "#                     # Train-test split for the partition\n",
    "#                     X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "#                         X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "#                     )\n",
    "\n",
    "#                     # StratifiedKFold for 5-fold cross-validation\n",
    "#                     k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "#                     cross_val_results = cross_val_score(dt_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "#                     avg_cross_val = np.mean(cross_val_results)\n",
    "#                     trial_results[\"CV Accuracy\"].append(avg_cross_val)\n",
    "\n",
    "#                     # Fit the model on the entire training set\n",
    "#                     dt_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "#                     # Evaluate on the training set\n",
    "#                     y_train_full_pred = dt_model.predict(X_train_full)\n",
    "#                     train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "#                     trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "\n",
    "#                     # Evaluate on the test set\n",
    "#                     y_test_pred = dt_model.predict(X_test)\n",
    "#                     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#                     trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "\n",
    "#                 # Compute average accuracies over trials\n",
    "#                 avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "#                 avg_cross_val = np.mean(trial_results[\"CV Accuracy\"])\n",
    "#                 avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "\n",
    "#                 # Update the best hyperparameter if this criterion results in better CV accuracy\n",
    "#                 if avg_cross_val > opt_decision_cross_val:\n",
    "#                     opt_decision_cross_val = avg_cross_val\n",
    "#                     opt_criterion = criterion\n",
    "\n",
    "#                 # Output results\n",
    "#                 print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "#                 print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "#                 print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "#                 # Store results\n",
    "#                 results.append({\n",
    "#                     \"Dataset\": dataset_name,\n",
    "#                     \"Split\": split_name,\n",
    "#                     \"Classifier\": f\"Decision Tree (Criterion={criterion})\",\n",
    "#                     \"Train Accuracy\": avg_train,\n",
    "#                     \"CV Accuracy\": avg_cross_val,\n",
    "#                     \"Test Accuracy\": avg_test\n",
    "#                 })\n",
    "\n",
    "#             # After the trials, print the optimal hyperparameter for Decision Tree\n",
    "#             print(f\"  Optimal Hyperparameter for Decision Tree: Criterion={opt_criterion} with CV Accuracy={opt_decision_cross_val:.4f}\")\n",
    "\n",
    "# # Convert results list to DataFrame for easy analysis\n",
    "# results_df = pd.DataFrame(results)\n",
    "\n",
    "# # Output final results\n",
    "# print(\"\\n--- Cross-Validation Results ---\")\n",
    "# print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c601385-5d0c-4dd6-9371-1bd5d6edba3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Occupancy Dataset\n",
      "\n",
      "20/80\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.8426\n",
      "      Avg Cross-Validation Accuracy: 0.8155\n",
      "      Avg Test Accuracy: 0.8436\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.9894\n",
      "      Avg Cross-Validation Accuracy: 0.9894\n",
      "      Avg Test Accuracy: 0.9886\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.9893\n",
      "      Avg Cross-Validation Accuracy: 0.9893\n",
      "      Avg Test Accuracy: 0.9883\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.9893\n",
      "      Avg Cross-Validation Accuracy: 0.9893\n",
      "      Avg Test Accuracy: 0.9884\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.9893\n",
      "      Avg Cross-Validation Accuracy: 0.9893\n",
      "      Avg Test Accuracy: 0.9883\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=0.01\n",
      " Optimal Cross Validation Accuracy0.9894\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9917\n",
      "      Avg Test Accuracy: 0.9933\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9926\n",
      "      Avg Test Accuracy: 0.9929\n",
      "  Optimal Criterion for Decision Tree: Criterion=entropy\n",
      " Optimal Cross Validation Accuracy: 0.9926\n",
      "\n",
      "50/50\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.7865\n",
      "      Avg Cross-Validation Accuracy: 0.7716\n",
      "      Avg Test Accuracy: 0.7873\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.9895\n",
      "      Avg Cross-Validation Accuracy: 0.9895\n",
      "      Avg Test Accuracy: 0.9889\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.9895\n",
      "      Avg Cross-Validation Accuracy: 0.9894\n",
      "      Avg Test Accuracy: 0.9888\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.9895\n",
      "      Avg Cross-Validation Accuracy: 0.9895\n",
      "      Avg Test Accuracy: 0.9888\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.9894\n",
      "      Avg Cross-Validation Accuracy: 0.9895\n",
      "      Avg Test Accuracy: 0.9888\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=0.01\n",
      " Optimal Cross Validation Accuracy0.9895\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9902\n",
      "      Avg Test Accuracy: 0.9909\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9911\n",
      "      Avg Test Accuracy: 0.9921\n",
      "  Optimal Criterion for Decision Tree: Criterion=entropy\n",
      " Optimal Cross Validation Accuracy: 0.9911\n",
      "\n",
      "80/20\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7690\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.7690\n",
      "      Avg Cross-Validation Accuracy: 0.7690\n",
      "      Avg Test Accuracy: 0.7691\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.9896\n",
      "      Avg Cross-Validation Accuracy: 0.9887\n",
      "      Avg Test Accuracy: 0.9872\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.9910\n",
      "      Avg Cross-Validation Accuracy: 0.9908\n",
      "      Avg Test Accuracy: 0.9886\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.9907\n",
      "      Avg Cross-Validation Accuracy: 0.9907\n",
      "      Avg Test Accuracy: 0.9887\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.9907\n",
      "      Avg Cross-Validation Accuracy: 0.9908\n",
      "      Avg Test Accuracy: 0.9887\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=1\n",
      " Optimal Cross Validation Accuracy0.9908\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9898\n",
      "      Avg Test Accuracy: 0.9880\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9906\n",
      "      Avg Test Accuracy: 0.9884\n",
      "  Optimal Criterion for Decision Tree: Criterion=entropy\n",
      " Optimal Cross Validation Accuracy: 0.9906\n",
      "\n",
      "Sepsis Dataset\n",
      "\n",
      "20/80\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=1e-08\n",
      " Optimal Cross Validation Accuracy0.9263\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9260\n",
      "      Avg Test Accuracy: 0.9259\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 0.9264\n",
      "      Avg Cross-Validation Accuracy: 0.9260\n",
      "      Avg Test Accuracy: 0.9259\n",
      "  Optimal Criterion for Decision Tree: Criterion=gini\n",
      " Optimal Cross Validation Accuracy: 0.9260\n",
      "\n",
      "50/50\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=1e-08\n",
      " Optimal Cross Validation Accuracy0.9263\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 0.9265\n",
      "      Avg Cross-Validation Accuracy: 0.9256\n",
      "      Avg Test Accuracy: 0.9258\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 0.9265\n",
      "      Avg Cross-Validation Accuracy: 0.9256\n",
      "      Avg Test Accuracy: 0.9258\n",
      "  Optimal Criterion for Decision Tree: Criterion=entropy\n",
      " Optimal Cross Validation Accuracy: 0.9256\n",
      "\n",
      "80/20\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.9263\n",
      "      Avg Cross-Validation Accuracy: 0.9263\n",
      "      Avg Test Accuracy: 0.9263\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=1e-08\n",
      " Optimal Cross Validation Accuracy0.9263\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 0.9267\n",
      "      Avg Cross-Validation Accuracy: 0.9239\n",
      "      Avg Test Accuracy: 0.9248\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 0.9267\n",
      "      Avg Cross-Validation Accuracy: 0.9239\n",
      "      Avg Test Accuracy: 0.9248\n",
      "  Optimal Criterion for Decision Tree: Criterion=entropy\n",
      " Optimal Cross Validation Accuracy: 0.9239\n",
      "\n",
      "Coupon Dataset\n",
      "\n",
      "20/80\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.5684\n",
      "      Avg Cross-Validation Accuracy: 0.5684\n",
      "      Avg Test Accuracy: 0.5684\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.5684\n",
      "      Avg Cross-Validation Accuracy: 0.5684\n",
      "      Avg Test Accuracy: 0.5684\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.6464\n",
      "      Avg Cross-Validation Accuracy: 0.6315\n",
      "      Avg Test Accuracy: 0.6451\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.6928\n",
      "      Avg Cross-Validation Accuracy: 0.6844\n",
      "      Avg Test Accuracy: 0.6918\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.6950\n",
      "      Avg Cross-Validation Accuracy: 0.6865\n",
      "      Avg Test Accuracy: 0.6918\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.6948\n",
      "      Avg Cross-Validation Accuracy: 0.6868\n",
      "      Avg Test Accuracy: 0.6920\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.6947\n",
      "      Avg Cross-Validation Accuracy: 0.6868\n",
      "      Avg Test Accuracy: 0.6923\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=10000\n",
      " Optimal Cross Validation Accuracy0.6868\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 0.9986\n",
      "      Avg Cross-Validation Accuracy: 0.6783\n",
      "      Avg Test Accuracy: 0.6887\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 0.9986\n",
      "      Avg Cross-Validation Accuracy: 0.6775\n",
      "      Avg Test Accuracy: 0.6864\n",
      "  Optimal Criterion for Decision Tree: Criterion=gini\n",
      " Optimal Cross Validation Accuracy: 0.6783\n",
      "\n",
      "50/50\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.5684\n",
      "      Avg Cross-Validation Accuracy: 0.5684\n",
      "      Avg Test Accuracy: 0.5684\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.5684\n",
      "      Avg Cross-Validation Accuracy: 0.5684\n",
      "      Avg Test Accuracy: 0.5684\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.6163\n",
      "      Avg Cross-Validation Accuracy: 0.5961\n",
      "      Avg Test Accuracy: 0.6118\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.6936\n",
      "      Avg Cross-Validation Accuracy: 0.6788\n",
      "      Avg Test Accuracy: 0.6841\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.6947\n",
      "      Avg Cross-Validation Accuracy: 0.6782\n",
      "      Avg Test Accuracy: 0.6832\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.6945\n",
      "      Avg Cross-Validation Accuracy: 0.6779\n",
      "      Avg Test Accuracy: 0.6834\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.6945\n",
      "      Avg Cross-Validation Accuracy: 0.6779\n",
      "      Avg Test Accuracy: 0.6833\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=0.01\n",
      " Optimal Cross Validation Accuracy0.6788\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 0.9993\n",
      "      Avg Cross-Validation Accuracy: 0.6580\n",
      "      Avg Test Accuracy: 0.6742\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 0.9993\n",
      "      Avg Cross-Validation Accuracy: 0.6584\n",
      "      Avg Test Accuracy: 0.6711\n",
      "  Optimal Criterion for Decision Tree: Criterion=entropy\n",
      " Optimal Cross Validation Accuracy: 0.6584\n",
      "\n",
      "80/20\n",
      "Logistic Regression (C=1e-08)\n",
      "      Avg Train Accuracy: 0.5686\n",
      "      Avg Cross-Validation Accuracy: 0.5686\n",
      "      Avg Test Accuracy: 0.5684\n",
      "Logistic Regression (C=1e-06)\n",
      "      Avg Train Accuracy: 0.5686\n",
      "      Avg Cross-Validation Accuracy: 0.5686\n",
      "      Avg Test Accuracy: 0.5684\n",
      "Logistic Regression (C=0.0001)\n",
      "      Avg Train Accuracy: 0.5686\n",
      "      Avg Cross-Validation Accuracy: 0.5686\n",
      "      Avg Test Accuracy: 0.5688\n",
      "Logistic Regression (C=0.01)\n",
      "      Avg Train Accuracy: 0.7052\n",
      "      Avg Cross-Validation Accuracy: 0.6772\n",
      "      Avg Test Accuracy: 0.6743\n",
      "Logistic Regression (C=1)\n",
      "      Avg Train Accuracy: 0.7043\n",
      "      Avg Cross-Validation Accuracy: 0.6778\n",
      "      Avg Test Accuracy: 0.6714\n",
      "Logistic Regression (C=100)\n",
      "      Avg Train Accuracy: 0.7041\n",
      "      Avg Cross-Validation Accuracy: 0.6780\n",
      "      Avg Test Accuracy: 0.6715\n",
      "Logistic Regression (C=10000)\n",
      "      Avg Train Accuracy: 0.7041\n",
      "      Avg Cross-Validation Accuracy: 0.6780\n",
      "      Avg Test Accuracy: 0.6717\n",
      "  Optimal Regularization Parameter for Logistic Regression: C=100\n",
      " Optimal Cross Validation Accuracy0.6780\n",
      "    Classifier: Decision Tree (Criterion=gini)\n",
      "      Avg Train Accuracy: 0.9996\n",
      "      Avg Cross-Validation Accuracy: 0.6270\n",
      "      Avg Test Accuracy: 0.6407\n",
      "    Classifier: Decision Tree (Criterion=entropy)\n",
      "      Avg Train Accuracy: 0.9996\n",
      "      Avg Cross-Validation Accuracy: 0.6276\n",
      "      Avg Test Accuracy: 0.6452\n",
      "  Optimal Criterion for Decision Tree: Criterion=entropy\n",
      " Optimal Cross Validation Accuracy: 0.6276\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 3 splits x 3 classifiers x 3 datasets x 3 trials\n",
    "splits = {\n",
    "    \"20/80\": 0.2,  # 20% test, 80% train\n",
    "    \"50/50\": 0.5,  # 50% test, 50% train\n",
    "    \"80/20\": 0.8   # 80% test, 20% train\n",
    "}\n",
    "\n",
    "#To be later initialized so can tune hyper parameters\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": None,\n",
    "    \"Random Forest\": None,\n",
    "    \"Decision Tree\": None\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"Occupancy Dataset\": (X_occupancy, y_occupancy),\n",
    "    \"Sepsis Dataset\": (X_sepsis, y_sepsis),\n",
    "    \"Coupon Dataset\": (X_coupons, y_coupons)\n",
    "}\n",
    "\n",
    "num_trials = 3\n",
    "\n",
    "#results store values for later use when creating heatmap\n",
    "results = []\n",
    "\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "    \n",
    "    for split_name, test_size in splits.items():\n",
    "        print(f\"\\n{split_name}\")\n",
    "        \n",
    "        if \"Logistic Regression\" in classifiers:\n",
    "            optimal_log_param = None\n",
    "            opt_log_cross_val = -1  # dummy value place holder\n",
    "\n",
    "            #iterates through all possible regularization parameters\n",
    "            for regularization in [10**-8, 10**-6, 10**-4, 10**-2, 1, 10**2, 10**4]:\n",
    "                print(f\"Logistic Regression (C={regularization})\")\n",
    "                \n",
    "                log_model = LogisticRegression(max_iter=2000, solver=\"lbfgs\", C=regularization)\n",
    "\n",
    "     \n",
    "                trial_results = {\n",
    "                    \"Train Accuracy\": [],\n",
    "                    \"Cross Validation Accuracy\": [],\n",
    "                    \"Test Accuracy\": []\n",
    "                }\n",
    "\n",
    "                #Iterates for 3 trials as indicated\n",
    "                for trial in range(num_trials):\n",
    "                    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "                        X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "                    )\n",
    "                    \n",
    "                    # 5-fold cross validation\n",
    "                    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "                    cross_val_results = cross_val_score(log_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "                    avg_cross_val = np.mean(cross_val_results)\n",
    "                    trial_results[\"Cross Validation Accuracy\"].append(avg_cross_val)\n",
    "\n",
    "                    #training the model\n",
    "                    log_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "                    y_train_full_pred = log_model.predict(X_train_full)\n",
    "                    train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "                    trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "\n",
    "                    #evaluating tet\n",
    "                    y_test_pred = log_model.predict(X_test)\n",
    "                    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "                    trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "\n",
    "                # taking the average over the 3 trials\n",
    "                avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "                avg_cross_val = np.mean(trial_results[\"Cross Validation Accuracy\"])\n",
    "                avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "\n",
    "                # select optimal parameter \n",
    "                if avg_cross_val > opt_log_cross_val:\n",
    "                    opt_log_cross_val = avg_cross_val\n",
    "                    optimal_log_param = regularization\n",
    "\n",
    "                # Output results\n",
    "                print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "                print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "                print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "                results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Split\": split_name,\n",
    "                    \"Classifier\": f\"Logistic Regression (C={regularization})\",\n",
    "                    \"Train Accuracy\": avg_train,\n",
    "                    \"Cross Validation Accuracy\": avg_cross_val,\n",
    "                    \"Test Accuracy\": avg_test\n",
    "                })\n",
    "\n",
    "            print(f\"  Optimal Regularization Parameter for Logistic Regression: C={optimal_log_param}\\n Optimal Cross Validation Accuracy{opt_log_cross_val:.4f}\")\n",
    "\n",
    "        # if \"Random Forest\" in classifiers:\n",
    "        #     optimal_max_feat = None\n",
    "        #     opt_random_forest_cross_val = -1  \n",
    "            \n",
    "        #     # Iterate over different max_features values\n",
    "        #     for max_features in [1, 2, 4, 6, 8, 12, 16, 20]:\n",
    "        #         print(f\"    Classifier: Random Forest (max_features={max_features})\")\n",
    "        #         random_forest_model = RandomForestClassifier(n_estimators=1024, max_features=max_features, random_state=42)\n",
    "        \n",
    "        #         trial_results = {\n",
    "        #             \"Train Accuracy\": [],\n",
    "        #             \"Cross Validation Accuracy\": [],\n",
    "        #             \"Test Accuracy\": []\n",
    "        #         }\n",
    "        \n",
    "        #         for trial in range(num_trials):\n",
    "        #             X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        #                 X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "        #             )\n",
    "        \n",
    "        #             k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "        #             cross_val_results = cross_val_score(random_forest_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "        #             avg_cross_val = np.mean(cross_val_results)\n",
    "        #             trial_results[\"Cross Validation Accuracy\"].append(avg_cross_val)\n",
    "        \n",
    "        #             random_forest_model.fit(X_train_full, y_train_full)\n",
    "        \n",
    "        #             y_train_full_pred = random_forest_model.predict(X_train_full)\n",
    "        #             train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "        #             trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "        \n",
    "        #             y_test_pred = random_forest_model.predict(X_test)\n",
    "        #             test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        #             trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "        \n",
    "        #         avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "        #         avg_cross_val = np.mean(trial_results[\"Cross Validation Accuracy\"])\n",
    "        #         avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "        \n",
    "\n",
    "        #         if avg_cross_val > opt_random_forest_cross_val:\n",
    "        #             opt_random_forest_cross_val = avg_cross_val\n",
    "        #             optimal_max_feat = max_features\n",
    "        \n",
    "        #         print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "        #         print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "        #         print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "        \n",
    "        #         results.append({\n",
    "        #             \"Dataset\": dataset_name,\n",
    "        #             \"Split\": split_name,\n",
    "        #             \"Classifier\": f\"Random Forest (max_features={max_features})\",\n",
    "        #             \"Train Accuracy\": avg_train,\n",
    "        #             \"Cross Validation Accuracy\": avg_cross_val,\n",
    "        #             \"Test Accuracy\": avg_test\n",
    "        #         })\n",
    "        \n",
    "        #     print(f\"  Optimal Max Features for Random Forest: max_features={optimal_max_feat}, \\n Optimal Cross Validation Accuracy: {opt_random_forest_cross_val:.4f}\")\n",
    "\n",
    "        if \"Decision Tree\" in classifiers:\n",
    "            opt_criterion = None\n",
    "            opt_decision_cross_val = -1\n",
    "            \n",
    "            for criterion in [\"gini\", \"entropy\"]:\n",
    "                print(f\"    Classifier: Decision Tree (Criterion={criterion})\")\n",
    "                \n",
    "                dt_model = DecisionTreeClassifier(random_state=42, criterion=criterion)\n",
    "\n",
    "                trial_results = {\n",
    "                    \"Train Accuracy\": [],\n",
    "                    \"Cross Validation Accuracy\": [],\n",
    "                    \"Test Accuracy\": []\n",
    "                }\n",
    "\n",
    "                for trial in range(num_trials):\n",
    "                    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "                        X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "                    )\n",
    "\n",
    "                    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "                    cross_val_results = cross_val_score(dt_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "                    avg_cross_val = np.mean(cross_val_results)\n",
    "                    trial_results[\"Cross Validation Accuracy\"].append(avg_cross_val)\n",
    "\n",
    "                    dt_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "                    y_train_full_pred = dt_model.predict(X_train_full)\n",
    "                    train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "                    trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "\n",
    "                    y_test_pred = dt_model.predict(X_test)\n",
    "                    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "                    trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "\n",
    "                avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "                avg_cross_val = np.mean(trial_results[\"Cross Validation Accuracy\"])\n",
    "                avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "\n",
    "                if avg_cross_val > opt_decision_cross_val:\n",
    "                    opt_decision_cross_val = avg_cross_val\n",
    "                    opt_criterion = criterion\n",
    "\n",
    "                print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "                print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "                print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Split\": split_name,\n",
    "                    \"Classifier\": f\"Decision Tree (Criterion={criterion})\",\n",
    "                    \"Train Accuracy\": avg_train,\n",
    "                    \"Cross Validation Accuracy\": avg_cross_val,\n",
    "                    \"Test Accuracy\": avg_test\n",
    "                })\n",
    "\n",
    "            print(f\"  Optimal Criterion for Decision Tree: Criterion={opt_criterion}\\n Optimal Cross Validation Accuracy: {opt_decision_cross_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150a995-e13d-47c1-90c9-c90ef63ceed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Occupancy Dataset\n",
      "\n",
      "20/80\n",
      "    Classifier: Random Forest (max_features=1)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9938\n",
      "      Avg Test Accuracy: 0.9938\n",
      "    Classifier: Random Forest (max_features=2)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9937\n",
      "      Avg Test Accuracy: 0.9936\n",
      "    Classifier: Random Forest (max_features=4)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9936\n",
      "      Avg Test Accuracy: 0.9937\n",
      "    Classifier: Random Forest (max_features=6)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9933\n",
      "      Avg Test Accuracy: 0.9938\n",
      "    Classifier: Random Forest (max_features=8)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9932\n",
      "      Avg Test Accuracy: 0.9937\n",
      "    Classifier: Random Forest (max_features=12)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9932\n",
      "      Avg Test Accuracy: 0.9937\n",
      "    Classifier: Random Forest (max_features=16)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9932\n",
      "      Avg Test Accuracy: 0.9937\n",
      "    Classifier: Random Forest (max_features=20)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9932\n",
      "      Avg Test Accuracy: 0.9937\n",
      "  Optimal Max Features for Random Forest: max_features=1, \n",
      " Optimal Cross Validation Accuracy: 0.9938\n",
      "\n",
      "50/50\n",
      "    Classifier: Random Forest (max_features=1)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9928\n",
      "      Avg Test Accuracy: 0.9931\n",
      "    Classifier: Random Forest (max_features=2)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9926\n",
      "      Avg Test Accuracy: 0.9931\n",
      "    Classifier: Random Forest (max_features=4)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9925\n",
      "      Avg Test Accuracy: 0.9930\n",
      "    Classifier: Random Forest (max_features=6)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9923\n",
      "      Avg Test Accuracy: 0.9930\n",
      "    Classifier: Random Forest (max_features=8)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9922\n",
      "      Avg Test Accuracy: 0.9928\n",
      "    Classifier: Random Forest (max_features=12)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9922\n",
      "      Avg Test Accuracy: 0.9928\n",
      "    Classifier: Random Forest (max_features=16)\n",
      "      Avg Train Accuracy: 1.0000\n",
      "      Avg Cross-Validation Accuracy: 0.9922\n",
      "      Avg Test Accuracy: 0.9928\n",
      "    Classifier: Random Forest (max_features=20)\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 3 splits x 3 classifiers x 3 datasets x 3 trials\n",
    "splits = {\n",
    "    \"20/80\": 0.2,  # 20% test, 80% train\n",
    "    \"50/50\": 0.5,  # 50% test, 50% train\n",
    "    \"80/20\": 0.8   # 80% test, 20% train\n",
    "}\n",
    "\n",
    "#To be later initialized so can tune hyper parameters\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": None,\n",
    "    \"Random Forest\": None,\n",
    "    \"Decision Tree\": None\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"Occupancy Dataset\": (X_occupancy, y_occupancy),\n",
    "    \"Sepsis Dataset\": (X_sepsis, y_sepsis),\n",
    "    \"Coupon Dataset\": (X_coupons, y_coupons)\n",
    "}\n",
    "\n",
    "num_trials = 3\n",
    "\n",
    "#results store values for later use when creating heatmap\n",
    "results = []\n",
    "\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    print(f\"\\n{dataset_name}\")\n",
    "    \n",
    "    for split_name, test_size in splits.items():\n",
    "        print(f\"\\n{split_name}\")\n",
    "        \n",
    "        # if \"Logistic Regression\" in classifiers:\n",
    "        #     optimal_log_param = None\n",
    "        #     opt_log_cross_val = -1  # dummy value place holder\n",
    "\n",
    "        #     #iterates through all possible regularization parameters\n",
    "        #     for regularization in [10**-8, 10**-6, 10**-4, 10**-2, 1, 10**2, 10**4]:\n",
    "        #         print(f\"Logistic Regression (C={regularization})\")\n",
    "                \n",
    "        #         log_model = LogisticRegression(max_iter=2000, solver=\"lbfgs\", C=regularization)\n",
    "\n",
    "     \n",
    "        #         trial_results = {\n",
    "        #             \"Train Accuracy\": [],\n",
    "        #             \"Cross Validation Accuracy\": [],\n",
    "        #             \"Test Accuracy\": []\n",
    "        #         }\n",
    "\n",
    "        #         #Iterates for 3 trials as indicated\n",
    "        #         for trial in range(num_trials):\n",
    "        #             X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        #                 X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "        #             )\n",
    "                    \n",
    "        #             # 5-fold cross validation\n",
    "        #             k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "        #             cross_val_results = cross_val_score(log_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "        #             avg_cross_val = np.mean(cross_val_results)\n",
    "        #             trial_results[\"Cross Validation Accuracy\"].append(avg_cross_val)\n",
    "\n",
    "        #             #training the model\n",
    "        #             log_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "        #             y_train_full_pred = log_model.predict(X_train_full)\n",
    "        #             train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "        #             trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "\n",
    "        #             #evaluating tet\n",
    "        #             y_test_pred = log_model.predict(X_test)\n",
    "        #             test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        #             trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "\n",
    "        #         # taking the average over the 3 trials\n",
    "        #         avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "        #         avg_cross_val = np.mean(trial_results[\"Cross Validation Accuracy\"])\n",
    "        #         avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "\n",
    "        #         # select optimal parameter \n",
    "        #         if avg_cross_val > opt_log_cross_val:\n",
    "        #             opt_log_cross_val = avg_cross_val\n",
    "        #             optimal_log_param = regularization\n",
    "\n",
    "        #         # Output results\n",
    "        #         print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "        #         print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "        #         print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "        #         results.append({\n",
    "        #             \"Dataset\": dataset_name,\n",
    "        #             \"Split\": split_name,\n",
    "        #             \"Classifier\": f\"Logistic Regression (C={regularization})\",\n",
    "        #             \"Train Accuracy\": avg_train,\n",
    "        #             \"Cross Validation Accuracy\": avg_cross_val,\n",
    "        #             \"Test Accuracy\": avg_test\n",
    "        #         })\n",
    "\n",
    "        #     print(f\"  Optimal Regularization Parameter for Logistic Regression: C={optimal_log_param}\\n Optimal Cross Validation Accuracy{opt_log_cross_val:.4f}\")\n",
    "\n",
    "        if \"Random Forest\" in classifiers:\n",
    "            optimal_max_feat = None\n",
    "            opt_random_forest_cross_val = -1  \n",
    "            \n",
    "            # Iterate over different max_features values\n",
    "            for max_features in [1, 2, 4, 6, 8, 12, 16, 20]:\n",
    "                print(f\"    Classifier: Random Forest (max_features={max_features})\")\n",
    "                random_forest_model = RandomForestClassifier(n_estimators=1024, max_features=max_features, random_state=42)\n",
    "        \n",
    "                trial_results = {\n",
    "                    \"Train Accuracy\": [],\n",
    "                    \"Cross Validation Accuracy\": [],\n",
    "                    \"Test Accuracy\": []\n",
    "                }\n",
    "        \n",
    "                for trial in range(num_trials):\n",
    "                    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "                        X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "                    )\n",
    "        \n",
    "                    k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "                    cross_val_results = cross_val_score(random_forest_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "                    avg_cross_val = np.mean(cross_val_results)\n",
    "                    trial_results[\"Cross Validation Accuracy\"].append(avg_cross_val)\n",
    "        \n",
    "                    random_forest_model.fit(X_train_full, y_train_full)\n",
    "        \n",
    "                    y_train_full_pred = random_forest_model.predict(X_train_full)\n",
    "                    train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "                    trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "        \n",
    "                    y_test_pred = random_forest_model.predict(X_test)\n",
    "                    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "                    trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "        \n",
    "                avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "                avg_cross_val = np.mean(trial_results[\"Cross Validation Accuracy\"])\n",
    "                avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "        \n",
    "\n",
    "                if avg_cross_val > opt_random_forest_cross_val:\n",
    "                    opt_random_forest_cross_val = avg_cross_val\n",
    "                    optimal_max_feat = max_features\n",
    "        \n",
    "                print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "                print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "                print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "        \n",
    "                results.append({\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Split\": split_name,\n",
    "                    \"Classifier\": f\"Random Forest (max_features={max_features})\",\n",
    "                    \"Train Accuracy\": avg_train,\n",
    "                    \"Cross Validation Accuracy\": avg_cross_val,\n",
    "                    \"Test Accuracy\": avg_test\n",
    "                })\n",
    "        \n",
    "            print(f\"  Optimal Max Features for Random Forest: max_features={optimal_max_feat}, \\n Optimal Cross Validation Accuracy: {opt_random_forest_cross_val:.4f}\")\n",
    "\n",
    "        # if \"Decision Tree\" in classifiers:\n",
    "        #     opt_criterion = None\n",
    "        #     opt_decision_cross_val = -1\n",
    "            \n",
    "        #     for criterion in [\"gini\", \"entropy\"]:\n",
    "        #         print(f\"    Classifier: Decision Tree (Criterion={criterion})\")\n",
    "                \n",
    "        #         dt_model = DecisionTreeClassifier(random_state=42, criterion=criterion)\n",
    "\n",
    "        #         trial_results = {\n",
    "        #             \"Train Accuracy\": [],\n",
    "        #             \"Cross Validation Accuracy\": [],\n",
    "        #             \"Test Accuracy\": []\n",
    "        #         }\n",
    "\n",
    "        #         for trial in range(num_trials):\n",
    "        #             X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        #                 X, y.values.ravel(), test_size=test_size, random_state=42 + trial, stratify=y\n",
    "        #             )\n",
    "\n",
    "        #             k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42 + trial)\n",
    "        #             cross_val_results = cross_val_score(dt_model, X_train_full, y_train_full, cv=k_fold, scoring=\"accuracy\")\n",
    "        #             avg_cross_val = np.mean(cross_val_results)\n",
    "        #             trial_results[\"Cross Validation Accuracy\"].append(avg_cross_val)\n",
    "\n",
    "        #             dt_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "        #             y_train_full_pred = dt_model.predict(X_train_full)\n",
    "        #             train_accuracy = accuracy_score(y_train_full, y_train_full_pred)\n",
    "        #             trial_results[\"Train Accuracy\"].append(train_accuracy)\n",
    "\n",
    "        #             y_test_pred = dt_model.predict(X_test)\n",
    "        #             test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        #             trial_results[\"Test Accuracy\"].append(test_accuracy)\n",
    "\n",
    "        #         avg_train = np.mean(trial_results[\"Train Accuracy\"])\n",
    "        #         avg_cross_val = np.mean(trial_results[\"Cross Validation Accuracy\"])\n",
    "        #         avg_test = np.mean(trial_results[\"Test Accuracy\"])\n",
    "\n",
    "        #         if avg_cross_val > opt_decision_cross_val:\n",
    "        #             opt_decision_cross_val = avg_cross_val\n",
    "        #             opt_criterion = criterion\n",
    "\n",
    "        #         print(f\"      Avg Train Accuracy: {avg_train:.4f}\")\n",
    "        #         print(f\"      Avg Cross-Validation Accuracy: {avg_cross_val:.4f}\")\n",
    "        #         print(f\"      Avg Test Accuracy: {avg_test:.4f}\")\n",
    "\n",
    "        #         # Store results\n",
    "        #         results.append({\n",
    "        #             \"Dataset\": dataset_name,\n",
    "        #             \"Split\": split_name,\n",
    "        #             \"Classifier\": f\"Decision Tree (Criterion={criterion})\",\n",
    "        #             \"Train Accuracy\": avg_train,\n",
    "        #             \"Cross Validation Accuracy\": avg_cross_val,\n",
    "        #             \"Test Accuracy\": avg_test\n",
    "        #         })\n",
    "\n",
    "        #     print(f\"  Optimal Criterion for Decision Tree: Criterion={opt_criterion}\\n Optimal Cross Validation Accuracy: {opt_decision_cross_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488e7b56-9c4e-4a6c-a720-f9f30058c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi geenaaaaa. see u at gradddd."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
